{"meta":{"title":"blog","subtitle":"","description":"","author":"Yifei Dai","url":"81.68.232.125","root":"/"},"pages":[],"posts":[{"title":"yarn","text":"历史太古时期，npm 无道，众 JS 开发者苦 npm 久矣。npm 承平已久，不思进取。 这里就历数一下 npm 当年的几大原罪 速度慢 解析出来的依赖列表不稳定 安全性差 树状的安装结构 命令行输出极其难懂 … 故开源社区揭竿而起。其中的佼佼者就是 facebook 的 yarn。 yarn针对上述的缺点，yarn 进行了一系列的改进 使用 yarn.lock 文件稳定构建的版本 每次安装依赖使用 checksum 检查 使用扁平的安装结构 支持 workspace 易懂的命令行输出 … 当然以上这些优化在 yarn 的鞭策之下，npm 也同样进行了改进 使用 package-lock.json 文件稳定构建的版本 每次安装依赖使用 SHA-512 检查 … 不过，yarn 还是有一个相对于 npm 无可比拟的优势。yarn 的安装是并行的，而 npm 的安装是串行的。也就是说 npm 在安装完一个包后才能安装下一个包，而 yarn 可以同时安装多个包。 一些有用的 yarn 的小知识yarn why可以知道具体某一个包为何被安装 yarn upgrade-interactive使用可视化的界面来选择所有的依赖是否升级 yarn link用于本地包的开发，将本地的一个文件夹作为一个包，然后可以被另一个项目进行使用 yarn bin可执行文件安装的位置，yarn 的 bin 目录经常不在 PATH 中被设置，所以我们可以在 .bashrc 文件中加入 PATH=&quot;$PATH:(yarn global bin)&quot; 来使通过 yarn 全局安装的文件可以被直接执行 全局的缓存文件夹yarn 的文件下载后是先会被存在一个缓存的文件夹下，然后再拷贝进入 node_modules 目录，这个文件夹可以通过 yarn cache dir 命令找到。当一个文件已经被安装过，再次通过 yarn 这个命令进行安装，那么将不会通过网络下载这个包，而是直接从缓存中拷贝进入 node_modules 目录 offline mode有时候，我们如果不希望通过网络下载包，比如在弱网环境，或者由于公司原因的网络隔离等等。我们就可以开启 offline mode 在当前项目目录下新建 .yarnrc 文件 yarn-offline-mirror &quot;./npm-packages-offline-cache&quot; yarn-offline-mirror-pruning true这样我们就开启了 offline mode 这时候，如果我们运行 yarn 命令，我们会发现在 npm-packages-offline-cache 目录下多了和全局缓存文件夹下一样的缓存文件这时候我们再删除 node_modules 文件夹，断开网络，然后再执行 yarn 命令，我们仍然可以正常运行 yarn2随着越来越多的新功能的加入，yarn 原来的架构无法满足新需求，所以 yarn 推出了 v2 版本。他们做出了以下的变动 从 flow 迁移到 ts 进行开发 使用基于插件和模块化的代码架构，让开发者不用理解 yarn 的核心代码就可以通过插件的方式为 yarn 增加新功能 他也引入了不少新的功能和改进 更友好的输出信息 我们可以看到，每行日志开头都有了错误码，我们可以快速的通过 错误码文档 定位到问题。对于每个包的操作关联到了一起，同时还各自高亮了包的名字和版本号 更好的 workspace 支持yarn2 将 workspace 变成了一等公民，这样可以更好的支持 Monorepository。也就是一个项目空间中包含多个小的不同的包。比如 babel 就是一个例子。 他提供了以下便利的功能 yarn add interactive mode，使用yarn add xxx -i，其他 workspace 可能已经使用了这个包的某个版本，可以让用户选择是否复用 yarn up： 一次更新所有 workspace 下的版本，同样也具有 interactive mode 更好的发布流程体验 yarn workspaces foreach： 所有 workspace 运行同一个命令 contraints，给所有 workspace 增加一些约束规则，这样可以通过命令来检查 workspace 是否违反了这些规则 像搜索数据库一样查询workspaces的依赖信息，可以使用 yarn constraints query 命令查询 workspace 用到的依赖信息 yarn dlxyarn dlx 是 (download and execute) 的简称，他相当于 npx 的功能，将下载一个脚本并运行这个脚本。由于有缓存的原因，当执行过一次 yarn dlx 后再次执行会使用缓存的文件，速度会更快。 Normalized shellyarn2 对 Windows 开发环境做了更好的兼容。我们经常会遇到这样一个问题。一个脚本在 Mac 或 linux 环境中可以正常运行，但是在 Windows 上却无法正常运行。这也是我将开发环境迁移到 wsl 的原因。但是 yarn2 自带了一个简单的 shell 解析器，覆盖了90%常用的shell脚本写法，可以让脚本在 Windows 和其他系统中一样正常运行。 Protocols在 package.json 中 dependencies 和 devDependencies 字段中，我们可以定义我们的依赖，其中 key 是包名，而 value 指就涉及到了如何解析这个包。protocols 就是约定这个 value 值的语义是如何的 名字 例子 描述 Semver ^1.2.3 Resolves from the default registry Tag latest Resolves from the default registry Npm alias npm:name@… Resolves from the npm registry Git git@github.com:foo/bar.git Downloads a public package from a Git repository GitHub github:foo/bar Downloads a public package from GitHub GitHub foo/bar Alias for the github: protocol File file:./my-package Copies the target location into the cache Link link:./my-folder Creates a link to the ./my-folder folder (ignore dependencies) Patch patch:left-pad@1.0.0#./my-patch.patch Creates a patched copy of the original package Portal portal:./my-folder Creates a link to the ./my-folder folder (follow dependencies) Workspace workspace:* Creates a link to a package in another workspace Exec exec:./my-generator-package Experimental &amp; Plugin. Instructs Yarn to execute the specified Node script and use its output as package content link 和 portal 的区别？ 他们都是指向本地文件系统中的一个 symlink。但是当那个指向的包中包含 package.json 文件的时候，yarn 会解析这个包中的 transitive dependencies（也就是依赖的依赖） Zero-Installs这是 yarn2 带来的一种新的理念。我们平时都会使用 yarn 安装，但是 Zero-Install 是指当你从 git 拉取别人的代码后，你无需运行 yarn 命令即可立即运行。 在开发的时候，我们经常会碰到一个问题，拉取代码后由于别人引入了一个新的依赖就无法运行了。这就可以解决这个问题 yarn2 认为，根据墨菲定律，任何可能出问题的东西最终都可能会出问题。比如 yarn 出了 bug，网络环境有问题等等。所以为了防止这个，我们就最好运行最少量的代码。 Zip loading我们想要实现 Zero-Installs，就必然会面临一个问题一个问题，node_modules 中有海量的文件，巨量的文件让我们不可能把这些文件推送的远程。所以 yarn2 把每个包都各自压缩成了一个 .zip 的文件，PnP 的 runtime(.pnp.cjs) patch 了 fs 模块，让我们可以访问 Zip 文件中打包的文件。 PnP(Plug’n’Play)这个功能我觉得是 yarn2 主推的一个新功能，也是和我们日常开发最息息相关，提升体验巨大的一个功能。所以我单独分开来讲他。就是通过 PnP 实现了之前提到的理念 Zero-Installs。当然如果你不同意这个理念，你也可以通过一些设置关闭它。总得来说，他大幅度的提升了打包的速度。 node_modules 的问题我们平时的 yarn 命令执行后会生成一个 node_modules 文件夹。通过 node 的 resolve 算法，我们可以使用 node_modules 中按照的包。但是因为很多原因，这个过程非常的低效： node_modules 文件夹中包含海量的文件。yarn 命令中超过 70% 的时间都是花在这个文件夹上面。即使有了已经安装的文件也不行，包管理器要检查当前目录中的文件和最终文件的差异 由于生产 node_modules 文件夹是一个重 IO 的操作，所以包管理器没有太多的余地去优化他，因为他只是做了拷贝文件的操作 node 并没有包的概念。他并不知道一个文件是否要被使用。完全可能出现这样的情况，在本地开发环境你的代码可以正常运行，但是由于在忘记向 package.json 文件中添加某个依赖，导致你的项目不能运行了 即使在运行时，node 也需要进行许多访问文件的操作去找出这个文件的实际位置在哪里（因为导入依赖时写的路径可以有多种解析的结果，node会一一尝试），这很浪费时间 node_modules 的设计让包管理器很好的去减少重复包的引入（多个依赖的依赖是同一个包）。即使一些算法可以优化树状的文件结构（把所有的依赖都提升到最高级的目录），但是我们仍然可能会无法优化某些特定的情形，从而导致占用更多的存储空间，同一个包出现多次。 pnpm 解决 node_modules 问题的尝试这里先不说 yarn 是怎么解决这个问题的，我们先来看看社区的一些其他的解决方案，pnpm 就是这样的一个库。他也是一个包管理的库。 他和 yarn 以及 npm 不同，他的 node_modules 的文件夹结构仍然是树形的。这样做有一个好处，就是我们访问 node_modules 下的包时，我们就仅仅可以访问我们添加在 package.json 的依赖中的包，而不像 yarn 和 npm 一样，提升到最高层级的依赖的依赖也可以访问。 但是这样就有一个我们之前提到过的问题，当多个包依赖同一个包，或者同一个包的不同版本时，会大量增加磁盘的使用。为了解决这个问题，pnpm 在存储多个不同版本的同一个包时，仅仅会多存储那些发生改变过的文件。就比如对于一个包的 v1 和 v2，他们各自有 100 个文件，其中 99 个文件内容相同，1 个文件不同，pnpm 只会存储 101 个文件，99 个相同的文件，和两个版本各自 不同的文件。 除此之外所有项目的依赖的文件都将存储于同一个位置，他们通过 hard link 的方式存放到各自项目的 node_modules 文件夹下。 PnP 的解决方式yarn 知道依赖的包的安装位置，不仅如此，他还负责安装这些包。所以，为什么不是由 yarn 来负责定位寻找和管理依赖的安装位置呢？ 从 yarn2.0 开始，yarn 会生成一个 .pnp.cjs 的文件，这个文件包含了以下两个映射关系： 某个特定版本包名字映射他的安装位置 某个特定版本包名字映射他所有的子依赖的特定版本的包名字 通过这些信息， yarn 可以告诉 node 应该在哪里找到某一个包，只要他们是依赖树的一部分。所有的包都被安装在 .yarn/cahce 文件夹中。每个包都是一个 .zip 的文件。所以文件的数量大大减少，可以提交到 git 仓库，实现 Zero-Installs。通过 zip loading 的特性，我们可以访问这些 zip 文件。 这个解决方式有以下的优点 安装几乎是瞬时的，yarn 只需要生成一个文件而不是原来的成千上万的文件。主要的瓶颈由原来的磁盘性能变成了依赖的数量 由于减少了 IO 操作的数量，安装过程变的更加可靠。尤其是在 Windows 上，批量读写文件可能因为类似 Windows Defender 的工具引起未知的错误。node_modules 的重 IO 操作更可能会导致失败 对于依赖树完美的优化 生成的 pnp.cjs 文件可以被提交到你的仓库作为 Zero-Installs 的一部分 应用启动速度更快。node resolution 不需要像之前一样遍历整个文件系统 使用方法无论是在 yarn1 还是在 yarn2 中都可以使用 pnp 特性。 yarn1首先，你需要 yarn 1.12+ 版本 然后只需执行以下命令即可开启 PnP 特性 yarn --pnpyarn2yarn2 和 yarn1 不同。对于每一个单独的项目，都需要指定并锁定使用的 yarn 的版本。 这也是一个可以理解的事情。我们的项目是会在不同的环境上运行的，这些环境可能各自安装了不同版本的 yarn。这些 yarn 的版本之间完全可能会存在一定的 break changes。所以说，为了保证项目的稳定性，不仅仅依赖的版本应该被锁定，yarn 这个依赖管理工具的版本也应该被锁定。 我们先运行以下命令设定 yarn 的版本 yarn set version berry当然我们也可以把 berry 换成其他的值，来设定成其他的版本。 然后初始化 yarn 设置 yarn init在 gitignore 文件中加入一下代码 .yarn/* !.yarn/cache !.yarn/patches !.yarn/plugins !.yarn/releases !.yarn/sdks !.yarn/versions最后就可以通过 yarn add 安装依赖了。 生成的文件说明 .yarn/cache 和 .pnp.*：这两个可以被 gitignore，但是这就丧失了 Zero-Installs 的功能。.yarn/cahce 是包保存的位置，.pnp.* 是运行时文件，保存了包存储位置的映射信息 yarn/install-state.gz：这是一个不需要被 commit 的优化文件，他保存了你当前项目的状态，如果你运行下一个命令的时候，通过他可以节省一些时间 .yarn/patches: 保存了通过 yarn patch-commit 命令生产的 patchfiles .yarn/plugins，.yarn/releases：分别保存了我们在这个项目中引入的 yarn 插件以及我们使用 yarn set version 命令指定的管理当前项目的 yarn 的版本的源文件。这保证了我们在不同环境运行时使用的 yarn 都是一致的 .yarn/sdks：这里保存了由 @yarnpkg/sdks 生成的编辑器要用的 sdk，这个可以被 gitignore，不过这样你克隆项目的时候就需要重新安装这些 sdk .yarn/unplugged：这个文件夹保存了经过 unplug 之后的包 .yarn/versions：被 version plugin 使用存储一些包的发布信息 yarn.lock：保证各个包的版本以及下载位置的稳定 .yarnrc.yml：本项目 yarn 的配置文件 使用全局缓存在 .yarnrc.yml 文件中加入以下配置 enableGlobalCache: true这时，我们的依赖将不会被拷贝到本项目的 .yarn/cache 文件夹下，而是直接使用 yarn 的全局缓存文件夹 pnp loose mode我们知道，yarn 在 node_modules 目录中把属性结构的依赖树拉平成了单层的依赖列表，但是这个行为的结果是非标准化的且不稳定的，也就是说同一个依赖的结果运行多次是不稳定的。所以 PnP 禁止 require 那些不在 package.json 文件中的依赖。也就是说依赖的依赖是默认禁止被 require 的。但是，某些包可能会由此引发一些问题。 为了解决这个问题，yarn 推出了 loose mode。我们会生成一个 fallback pool，这个 pool 中包含了所有被提升到顶部的依赖。当我们 require 的东西不在依赖列表之中，我们会在 fallback pool 中寻找这个依赖。由于被提升到顶部的依赖的版本是不确定的，所以我们会报一个 warning。 要开启 loose mode，只需要在 .yarnrc.yml 文件中加入一下代码即可。 pnpMode: loose除此之外，要确保 .yarnrc.yml 文件中 nodeLinker: &quot;pnp&quot; 设置没有被修改（这是默认值） 其他注意事项由于 pnp 特性开启后，对于依赖的寻找是由 yarn 负责而不能直接由 node 负责，所以说在运行 node 脚本的时候不能直接通过 node xxx.js 来运行，否则可能会找不到依赖包的位置，需要引入pnp.cjs这个文件。可以由以下几个方法代替 执行 yarn node xxx.js 在要执行的脚本开头加入一行 require(&quot;./.pnp.cjs&quot;).setup() 的代码 执行 node -r ./.pnp.cjs ./xxx.js 执行 NODE_OPTIONS=&quot;--require $(pwd)/.pnp.cjs&quot; node ./xxx.js 兼容性目前大多数包都可以兼容 PnP 特性，但是仍然有一些例外。 其中 webpack4.x 可以通过 pnp-webpack-plugin 兼容。 以下是一些常见的但是不兼容的包的列表 angular flow react native pulumi VSCode Extension Manager Hugo ReScript 不过这并不意味着你在使用这些包的时候你就不能使用 PnP 了。你可以使用 node-modules plugin，具体操作流程可以看这个教程 缺陷由于我们在发生 resolution error 的时候发出的是 warning 而不是 error，所以应用无法捕获到。这意味着如果在一个 try/catch 中 require 一个 peer dependency，那么将报一个 warning，即使他不该这样（这里我不是很理解，照理应该是throw error，但这里只报 warning，怎么会没有影响？贴一个原文链接，可以自己领会）。唯一的 runtime 影响就是这会让人们感到困惑，但是我们可以安全的忽略他。 yarn unplug这个命令用于调试第三方包，以 lodash 为例，我们运行 yarn unplug lodash，lodash 包将从 .yarn/cache 文件夹下消失，转移到 .yarn/unplugged 文件夹下，并且文件由原来的 .zip 格式变成一个正常的文件夹，这样我们可以直接修改这个文件夹中的内容对第三方包进行调试。 除此之外，将会在 package.json 文件中新增一段配置 &quot;dependenciesMeta&quot;: { &quot;lodash@4.17.21&quot;: { &quot;unplugged&quot;: true } }如果调试完之后想要取消，在 package.json 文件中删除这段配置，然后执行 yarn 命令即可","path":"blog/uncategorized/yarn/"},{"title":"webpack5迁移记录","text":"webpack5 已经发布了好久了，但是我们现在项目构建使用的还是 webpack4。也是时候尝试一下使用 webpack5。感受一下 webpack5 有什么新的功能和改进了。 webpack5 相对于 webpack4 的提升提升 使用持久化缓存来提升构建性能 使用更好的算法和默认值来改进持久化缓存 使用更好的tree shaking和代码生成来减小打包的体积 改进web平台的兼容能力 在不造成breaking change的前提下改进一些遗留在webpack4中内部奇怪的结构 通过引入一些breaking changes来为未来的一些新需求做准备，这是为了可以尽可能长的将版本保持在v5 升级准备webpack 提供了专门的文档，迁移到webpack5，来从v4到v5进行升级。这是在官方文档中，他们建议的迁移过程。 Node.js 至少需要 10.13.0 版本 升级 plugin 和 loader 确认构建没有 error 和 warning 确保设置了 mode 选项 更新一些过时的配置，这个链接中展示了完整的过时的配置列表 测试 webpack5 的兼容性 将 webpack 的包更新到5 清理一些配置，这个链接中展示了完整的迁移过程中需要注意修改的配置的列表 尝试构建并解决构建的问题 升级实践看了一遍文档后，就要自己尝试实践一下这个过程，看看会遇到哪些问题了。我就以自己最近正负责的一个项目为例进行尝试，对照上述的步骤进行一一的尝试。 Node.js 至少需要 10.13.0 版本 这一步没有任何的问题，只是升级了本地的后别忘了升级构建生产包的服务器的 Node.js 版本。 升级 plugin 和 loader 我先运行了dev的编译命令，以下两个插件报错了，各自升级到最新版本后解决了问题 vue-loader webpack-dev-middleware html-webpack-plugin 移除optimize-css-assets-webpack-plugin，使用 css-minimizer-webpack-plugin 替代 确认构建没有 error 和 warning 我配置了 html-webpack-plugin 中 inject: head，插入的 script 标签中，自动加上了 defer 属性。所以我在 body 中手动引入的那些 script 也需要加上 defer 属性才可以保证执行顺序。 确保设置了 mode 选项 更新一些过时的配置 测试 webpack5 的兼容性 将 webpack 的包更新到5 清理一些配置 尝试构建并解决构建的问题","path":"blog/js/webpack5迁移记录/"},{"title":"wsl踩坑","text":"本文针对的是使用 wsl2 的开发者，记录的是我在使用 wsl 的过程中遇到的一些坑。欢迎大家一起来补充 wsl hot reload not workhttps://github.com/microsoft/WSL/issues/4417#issuecomment-526279339 注：在 wsl2 中，如果打开的文件是在 /mnt 下（也就是在Windows中下载），是无法 hot reload 的，可以在 linux 子系统中下载项目 wsl terminal .bashrc fileWindows 和 wsl2 的命令行使用的 bashrc 不是同一个文件。Windows 的是在 Windows 的用户目录下，wsl 在 ~/.bashrc。这两个不是一个位置。 wsl install nodehttps://github.com/nodejs/help/wiki/Installation#how-to-install-nodejs-via-binary-archive-on-linux Windows browse linux file system 可以通过在地址栏中输入 \\\\wsl$ 来访问 通过 wsl –mount 生成(preview 中，持续关注), https://docs.microsoft.com/en-us/windows/wsl/wsl2-mount-disk open current folder in terminalexplorer.exe .为了方便，可以设置 alias alias explorer=&quot;explorer.exe .&quot;yarn global add not workhttps://stackoverflow.com/a/40333409 You should add export PATH=&quot;$PATH:$(yarn global bin)&quot; to your ~/.bash_profile or whatever you use. wsl vmmem 进程内存占用过大我开了4个VSCode，结果电脑内存直接炸了，一查，vmmem进程占了5G https://blog.n0ts.cn/1155.html win+R,输入%UserProfile%，会打开一个文件夹，在其中新建一个.wslconfig,其中输入一下内容 [wsl2] memory=4GB swap=2GB localhostForwarding=truememory为系统内存上限，可根据实际情况进行配置 运行wsl --shutdown命令，关闭wsl的运行 然后运行wsl命令，重启wsl wsl2 set proxy因为 shadowSocks 不是运行在 wsl2 本地，所以得通过 IP 访问 Windows。Windows 的 IP 对于 wsl2 是动态的，可以通过一个脚本来获取。在 ~/.bashrc 中加入以下配置。注意，在代理软件中的设置中打开 允许来自局域网的链接 选项 export HOST_IP=$(grep -oP &#39;(?&lt;=nameserver\\ ).*&#39; /etc/resolv.conf) export PROXY_ADDR=&quot;http://$HOST_IP:1080&quot; yarn config set proxy &quot;$PROXY_ADDR&quot; yarn config set https-proxy &quot;$PROXY_ADDR&quot; npm config set proxy &quot;$PROXY_ADDR&quot; npm config set https-proxy &quot;$PROXY_ADDR&quot; export http_proxy=$PROXY_ADDR export https_proxy=$PROXY_ADDR如果，还是无法成功连接，可能是因为 Windows 的防火墙的原因。按 win 按钮打开开始菜单，搜索 允许应用通过 Windows Defender 防火墙。更改其中的配置，寻找你的代理程序（对我是 ShadowSockR)，把所有的勾都打上。应该就可以解决了。 局域网访问自动化脚本映射端口（需提前配置需要映射的端口）：https://github.com/microsoft/WSL/issues/4150#issuecomment-504209723 直接的命令： # $addr: 0.0.0.0, $remoteport: wsl2 的内网地址，可通过 ipconfig 查看 netsh interface portproxy add v4tov4 listenport=$port listenaddress=$addr connectport=$port connectaddress=$remoteport解除 windows defender 影响https://github.com/microsoft/WSL/issues/4139#issuecomment-732067409","path":"blog/uncategorized/wsl2踩坑/"},{"title":"JS垃圾回收机制","text":"有些语言，比如C语言，是需要程序员手动申请和释放内存的，这增加了程序员的心智负担。大多数语言都提供了自动的垃圾处理机制。JS也同样如此。 垃圾回收机制JS 垃圾回收机制的根本就是判断一个变量是否可以被访问到。如果一个变量不可以从根上开始，由某种路径访问到的话，那么这个变量就是不可被访问的。这种情况下，JS引擎就认为这个变量是可以被垃圾回收的。 function a() { const f = {} } function b() { const g = {} return g } const d = a() const e = b()简单的举一个例子。我们看一下上述代码。 在执行a函数期间，变量f始终存在，因为f可以在当前的上下文环境，也就是a函数中被访问到。但是当a函数执行结束，我们的上下文环境变成了全局，这时候我们就无法通过任何方式访问到f变量了。这时候JS引擎就把e变量给释放掉了。 在执行b函数期间，变量g始终存在，因为g可以在当前的上下文环境，也就是b函数中被访问到。当b函数执行结束，将g作为返回值赋值给了e，所以即使上下文环境切换了，但是我们仍然可以通过变量e来访问变量g，所以变量g并不会被回收。 一般垃圾回收的算法是通过 标记-清除 来实现的。垃圾回收期会定时的执行这些任务。 获取根并标记。 也就是我们的全局环境。 标记所有根可以访问到的引用。 也就是全局变量，以及各个代码块形成的局部环境。 标记上一步标记的引用可以访问到的引用，并不断重复这一步，直到没有新的未被标记的变量出现为止。 回收所有没有被标记到的变量。 内存泄露的原因 全局变量 由于全局变量是直接存在根环境之中的，所以显然他是在任何情况下都不会被回收的。我们要尽可能少的使用全局变量。 当然有时候我们会无意中使用了全局变量，如下所示： function a() { b = 1 }在上述代码中，变量 b 将作为一个全局变量进行定义。这显然是我们不希望的。当然，现在使用linter或者开启JS的严格模式都很容易可以避免这个问题的发生。 闭包 在某一个函数执行完后，照理这个函数环境下定义的变量都将要被回收，但是有一个情况是例外的，那就是闭包。 在闭包的情况下，函数中的那个变量将可以被外界所访问到，所以显然是不能被回收的。 忘记结束的定时器 setInterval(function a() { }, 1000)这个函数a将用于不会被回收，因为这个定时器将一直被执行，所以显然不能被回收，所以我们要记住结束定时器。 window.addEventListener(&quot;click&quot;, function a() { })同样的，这种监听器也是如此。不能忘记 removeEventListener。 被移除的DOM 大家在操作DOM的时候，有些DOM节点可能会被多次使用。这时候，我们会考虑把DOM的查询结果存在一个变量之中。因为我们都知道，DOM和JS引擎是互相独立的，JS要操作DOM是很耗性能的。那么如果之后我们把这个DOM移除了，这个DOM节点其实并没有被回收，因为他还被一个变量所引用了。 console.log 被 console.log 所打印的变量是不会被回收的。这也很好理解，因为我们要随时可以去 console 里面看这个变量，所以当然不能被回收。所以在生产环境中，我们要尽量避免使用 console.log。一般生产环境的打包都有自动移除 console 的选项。 如何发现内存泄露（待补图）首先第一步是要确定是否存在内存泄露的现象。我们可以靠 Chrome 提供的调试工具来做到这个。 首先按 F12 打开 Chrome 的调试面板。 点击跳转到 Performance 这个 tab。 点击左上方的实心小圆球，hover 上去显示的是 record 的那个。点击这个将开始记录之后的各种性能问题的参数。 进行一些认为存在性能问题的操作。 点击 Stop 按钮。结束记录。 在最上面，我们可以看到 FPS，CPU，NET，HEAP 这四个性能指标的图表图表。内存问题的话可以看 HEAP 这个。如果他的趋势是一直上升那显然就存在内存泄露。 详细的内容也可以给 ScreenShots 旁边的 Memory 打钩（注意不是和 Performance 同级的那个，在下一行中），打开这个新的面板。可以看到内存随着时间的使用详情。 但是这个工具只能记录一段时间的使用情况。Chrome 有一个工具，名字叫 Performance Monitor 可以实时的监控各种性能指标的使用情况。打开方式如下： 点击调试工具最上方的最右边的 X 左边的三个点，会弹出一个 context menu 鼠标悬浮到这个弹出的菜单的 More tools 上，会再延伸出一个子菜单 点击子菜单中的 Performance Menu，将在最下方弹出 Performance Menu 菜单 我们可以看到，这个菜单中有 CPU usage， JS heap size， DOM Nodes 等众多性能指标，并且在实时的进行记录。通过这个工具，我们可以更加方便的进行性能分析。 最后，我们就要分析到底是什么导致了内存泄露。 这时候，我们就要点开最上面的 Memory 这个 tab 了。 点开这个菜单后，点击左上角的小圆球（hover上去显示 Take heap snapshot）。点击这个后将生成一个那个时刻的内存使用情况的快照。我们可以生成多份快照，来比较不同时间之间内存使用的变化来分析哪里发生了内存泄露。当然，我们也可以点击那个垃圾桶的图标，来进行手动的触发垃圾回收。再生成新的快照来比较内存使用情况进行分析。 再让我们点开一份快照，看看里面有哪些信息。我们可以看到，他分成了两个窗口，上面有很多条目，下面的标题是 Retainer（可以理解成定位器），但是空无一物。 观察一下这些条目，发现有 array closure compiled code concatenated string number regexp … 可以看到，他把内存的使用情况进行分类的整理。随便点开一个类型，可以看到很多“变量”。他们都各自在当前实际占用了一部分内存。点击选中一个“变量”，我们可以发现，底下的那个 Retainer 窗口有值了。显示出来的是一个嵌套的结构。注意，这个嵌套结构是反的。这个嵌套的结构展示了，这个变量是如何通过root来访问到他的。也就是说最上层是自己，下层就是他自己的上级。这个结构就展示了为什么这个变量没有被回收。而且，更方便的是，在每一层的右边，有一个代码的链接，可以让你迅速的找到创建这一层上下文或者变量的代码的位置。 通过对这两个窗口进行分析，我们可以比较清楚的定位到发生内存泄露的位置了。","path":"blog/uncategorized/JS垃圾回收机制/"},{"title":"Canvas 的几个坑","text":"最近接到公司的一个临时需求，要生成一批不同名字的邀请函。一开始我觉得这实在是太简单了，不就是canvas上贴一张图再写名字就结束了吗？但是没想到，因为不熟悉canvas导致这其中好几个坑花了我好多时间。 fillText 参数接到这个需求，上来肯定是先去看看写字的api喽。然后我就开始测试一下。 ctx.fillText(&quot;hello&quot;, 0, 0)咦，怎么canvas上啥都没有啊？这个api我用错了吗？后来我发现我确实用错了。后两个参数代表的点是文字的左下角而不是左上角。这也很可以理解，因为绘制文字的时候总是以下方的线进行对齐绘制的。 canvas 的字体为了让邀请函比较好看，当然是要对字使用某些比较好看的字体啦。但是，我却发现我设置的字体参数没有任何效果。 咦？不会呀，我明明已经用 font-face 定义了这些字体了啊。到底问题在哪里呢？我打开调试器发现，字体文件竟然都没有被下载？？？那一定是我 font-face 的定义格式写错了？ 最后发现并非如此，浏览器会对字体进行懒加载，只有当实际使用这个字体的时候才会下载字体文件。所以说只定义 font-face 就在 canvas 中使用这个字体是没有效果的，因为这个字体文件都没有下载呢。这时候有两个办法： 在 DOM 中事先使用一次这个字体，让这个字体文件先加载了。这种方法有比较大的限制。需要保证在 canvas 中使用这个字体的时候字体文件已经加载完成了。 使用 JS 来加载这个字体。话不多说上代码 // 像font-face中一样定义这个字体 const font = new FontFace(&quot;Arial&quot;, &quot;url(Arial.woff)&quot;, { style: &quot;normal&quot;, weight: 700 }) // 下载字体 font.load() .then(function(loadedFace) { // 把定义的字体加入文档流 document.fonts.add(loadedFace) }).catch(function(error) { // error }) 有一个字体加载的回调函数： document.fonts.ready.then(function(fontFaceSet) { // all fonts have been loaded })获取文字的高度由于要计算文字放置的位置，所以就需要获取文字的宽高。这时候就找到了 measureText 这个 api 了。但是之后才发现，这个 api 并不能获取文字的高度，只能获取宽度。 经过查找，发现并不存在直接获取高度的api。准确的说，没有兼容度良好的获取高度的api。 const metrics = ctx.measureText(text) // 字体的高度，无论使用哪个字 const fontHeight = metrics.fontBoundingBoxAscent + metrics.fontBoundingBoxDescent // 某一个字的实际高度 const actualHeight = metrics.actualBoundingBoxAscent + metrics.actualBoundingBoxDescent通过 caniuse 查看兼容度发现，Chrome 要版本 87 后才支持。Firefox和Edge默认不支持，需要设置某些特殊的实验属性才能支持。Safari要版本14才开始支持。 网上常见的方法都是一些比较hack的方法。主要思路就是手动构造一段这个文字。然后测量这段文字的高度。测量的方式一种是在DOM中构造用JS进行测量，还有一种是在canvas中作图，转化成图片，根据图片的像素点进行测量。 绘制文字的参数这个很容易注意到，就提一句。当你绘制了一段文字后，要注意重置那些参数，否则下一段文字将要继续使用这些参数。 canvas 绘制层级由于图片加载是异步的，所以如果用同步的方式写代码图片总是后与文字画的。也就是说图片会叠加在文字之上。当时我想偷懒不想写异步代码，所以很自然想到 canvas 中会不会有类似 css 中 z-index 的属性。结果发现是不存在的，层级只能由绘制的先后关系决定。 canvas 撤销操作画上一张图，一段文字后生成了一张邀请函。这时候我就想要撤销写文字的操作。由于印象中 canvas 有 save 和 restore 的 api，就直接尝试了一下，发现不起作用。这时候我才知道 save 保存的是画布的坐标状态，而不是画布的全部状态。我又尝试找了一下canvas中撤销该怎么实现，发现并没有简单的实现方法。我看到的一共有以下几种方法： 记录一个操作栈，撤销后就把canvas清空，然后把之前的操作重复一遍。当步数过多的时候就可能存在性能问题 同样是记录一个操作栈，每次进行操作的时候，都记录一下要回撤这个操作所需要的操作是什么。这个比较难以实现，很难用程序写出所有的撤销应该怎么做。 记录一个 canvas 状态的栈，每一次都把canvas当前图片保存下来。当步数过多时会占用很大的内存。 所以还是需要根据程序的实际情况来做撤销的实现。当然我这里就直接清空重新画一遍是最方便的。 canvas 图片下载当邀请函生成后我就想要下载邀请函的图片了。我使用了 toDataURL 这个 api。然后把他放到一个HTML的链接标签 a 的 src 属性中。然后调用元素的 click 方法模拟点击进行下载。但是没想到下载的东西报了一个网络错误。经过调查，发现是因为某些浏览器中对于src属性中的值的长度有限制，如果太长就无法正常下载。为了解决这个问题，我找到了以下两个方法。 减小图片体积：toDataURL 这个 api 的第一个参数是 MIME 类型。这时候我们可以将其指定为 image/jpeg 来压缩图片的体积。如果这还不够，toDataURL 的第二个参数是图片的压缩程度。我们可以使用 toDataURL(&quot;image/jpeg&quot;, 0.5) 来继续缩小图片体积。 使用 blob 来进行下载。","path":"blog/canvas/Canvas-的几个坑/"},{"title":"优化webpack打包时间","text":"在开发一个项目的时候，一开始webpack进行编译的速度都是很快的，因为我们引入的依赖很少。但是，当你的项目越来越臃肿的时候，打包的时间将大幅度的上升，有时候达到几分钟这种完全不能接受的地步。这时候，我们就应该开始着手对于webpack的打包流程进行优化了。 分析依赖 想要知道什么东西影响了打包的时间，那么我们当然需要了解我们打包出来的文件的构成。是哪些依赖导致的我们打包速度变慢。这时候，我们就要用到 webpack-bundle-analyzer 这个插件了。在使用了这个插件后，会自动打开一个网页，里面的图片就是我们打包结果的构成。如下图所示。 显而易见的，我们可以发现，element 和 echarts 这两个库占用了大量的体积，我们可以对此进行优化。 上面的插件是用来分析每个依赖的体积的。但是，我们要关心的不止是体积，我们关注点的根源应该是时间。所以，我们需要一个东西来分析所有的依赖打包所花的时间。这时候，我们就要用到 speed-measure-webpack-plugin 这个东西了。这个东西的引入也很简单。 // webpack file const SpeedMeasurePlugin = require(&quot;speed-measure-webpack-plugin&quot;) const smp = new SpeedMeasurePlugin() module.exports = smp.wrap(webpackConfig)使用如上所示的代码把 webpack 配置包裹在这个函数中即可。 最后，我们运行命令进行打包可以看到如下图所示的结果。他们分别列出了每一个loader运行所花的时间。我们可以针对这些数据进行各自优化。 cache-loader 说到优化打包时间，那么我们第一时间想到的就是利用缓存，利用空间换时间了。第一次打包时在磁盘中存储一些打包后的结果，之后每次打包都可以利用这个结果那是极好的了。 cache-loader 就是用来做这件事情的东西。不过,第一次打包的时候可能要划更多的时间，因为需要打包一些需要缓存的内容。第二次之后就可以使用这些缓存的内容加快打包速度了。 配置他很简单，把他放在其他loader之前即可。 { test: /\\.vue$/, use: [ &quot;cache-loader&quot;, &quot;vue-loader&quot;, ], }我们仍然使用之前的项目进行测试。我们对于其中标红的两个包，babel-loaer 和 vue-loader 分别都配置了 cache-loader。第一次打包花了78s，第二次打包花了60s。 而当我们不使用 cache-loader 的时候则花了77s。 可以看到，使用了 cache-loader 后，第二次开始的打包时间有了明显的提升。 HardSourceWebpackPlugin HardSourceWebpackPlugin 同样是一个使用缓存来进行加速打包的一个插件。他的缓存默认被存在 /.cache/hard-source 文件夹中。第一次打包他将花费更多的时间以用于进行缓存。第二次打包则有显著的速度提升。 我们通用使用之前的项目进行测试。第一次打包花了149秒，第二次打包花了45秒。相比于cache-loader的时间有了不小的提升。 我们再尝试一下同时使用 cache-loader 和 HardSourceWebpackPlugin 进行打包。第一次打包花了156秒，第二次打包花了44秒。和单独使用 HardSourceWebpackPlugin 打包的速度并没有什么区别。 当然，在配置这个插件的时候还是有很多坑的。这些在这个插件的 README 中的 troubleshooting都有提到。 external external 是 webpack 中的一个配置项。这个配置项的功能是在打包的时候不对某些包进行打包。而是通过 CDN 的方式来引入这些包。这就是通过减少打包的内容来提高打包的时间的一种配置。 示例配置如下。 // in webpack config external: { jquery: &quot;jQuery&quot;, } // in code import $ from &quot;jquery&quot;这时候，这个 $ 就相当于引入 jQuery 了。 webpack dll dll 是 Windows 中的一个概念。这里用维基百科上话来解释一波。 所谓动态链接，就是把一些经常会共享的代码（静态链接的OBJ程序库）制作成DLL档，当可执行文件调用到DLL档内的函数时，Windows操作系统才会把DLL档加载存储器内，DLL档本身的结构就是可执行档，当程序有需求时函数才进行链接。通过动态链接方式，存储器浪费的情形将可大幅降低。静态链接库则是直接链接到可执行文件。DLL的文件格式与视窗EXE文件一样——也就是说，等同于32位视窗的可移植执行文件（PE）和16位视窗的New Executable（NE）。作为EXE格式，DLL可以包括源代码、数据和资源的多种组合。在更广泛的意义上说，任何同样文件格式的电脑文件都可以称作资源DLL。这样的DLL的例子有扩展名为ICL的图标库、扩展名为FON和FOT的字体文件。 所以，其实 dll 其实也是一种缓存的形式。在 webpack 中，他的使用方式是事先把一些第三方的库打包成一个个dll文件，在打包的时候就可以跳过这些文件进行打包。直接使用这些文件代码即可运行。 让我们来看看怎么使用这个东西。 首先要有一个生成 dll 文件的配置文件。 const vendors = [ &quot;vue&quot;, &quot;vue-router&quot;, &quot;vuex&quot;, ] module.exports = { entry: { vendor: vendors, }, output: { path: path.resolve(__dirname, &quot;../dll&quot;), filename: &quot;[name]_[fullhash].js&quot;, library: &quot;[name]_[fullhash]&quot;, }, plugins: [ new webpack.DllPlugin({ context: path.resolve(__dirname, &quot;../&quot;), name: &quot;[name]_[fullhash]&quot;, path: path.join(__dirname, &quot;manifest.json&quot;), }) ], }使用 webpack 运行上面的配置文件即可生成 dll 文件。生成的 dll 文件全都生成在了 dll 文件夹中。输出的配置文件则是 manifest.json new webpack.DllReferencePlugin({ context: path.resolve(__dirname, &quot;../&quot;), manifest: path.resolve(__dirname, &quot;manifest.json&quot;), })想要使用的这些 dll 文件只要在 webpack 配置中的 plugin 加上一个新的 plugin 即可。在里面配置一下之前生成的 dll 输出的配置文件 manifest.json 即可。 开启多进程 terser-webpack-plugin 可以开启多进程选项。这是一个压缩JS代码的差距。 thread-loader: 对于那些比较耗时的loader可以配置一下 thread-loader 开启多进程。配置也非常简单，只要把他放在想要配置的loader之前即可。但是进程的启动是需要时间的，所以最好只在耗时的loader上配置 thread-loader。否则可能会话更多的时间。 webpack 5 每次webpack 的升级都带来了打包性能不小的提升，所以升级你的webpack，一定可以得到不小的惊喜。","path":"blog/webpack/优化webpack打包时间/"},{"title":"stylus 变量管理","text":"这两天刚刚启动一个新项目，正好有一点时间来思考一些项目整体上的东西。其中我就遇到了一个问题。我们在使用 stylus 的时候，会定义一些变量，用这些变量来写css。比如说$font-base = 14px。但是，如果我们在 JS 中想要使用这个字体的大小，以前我只会直接在代码里面写一个14。再好一点，在函数中定义一个变量const FONT_BASE = 14。或者再开一个专门的文件来定义 JS 中使用的 stylus 的变量，用的时候再引入。 但是，这样的话是违背代码书写的原则的。既然 stylus 的变量定义和 JS 中我们使用的变量是同一个东西，那么，我们当然也希望可以使用一个文件就可以管理 stylus 变量的定义。 经过我的寻找，我发现 stylus 中确实存在这样一个方法来实现这个目标。 那就是 stylus 的 json 函数。具体的文档位置为 stylus json函数 我们可以把想要定义的 stylus 变量都先定义在一个 JSON 文件中，然后我们可以在 stylus 文件中使用 json 函数进行引入。而当我们想在 JS 中使用这些变量，也可以通过这个 JSON 文件引入。这样就实现了同一个文件来管理 stylus 变量的方法。我们看一个例子。 // variable.json { &quot;$main&quot;: &quot;#234235&quot;, &quot;$font&quot;: { &quot;base&quot;: &quot;14px&quot;, &quot;medium&quot;: &quot;16px&quot; } }// variable.styl json(&quot;./variable.json&quot;)这样做相当于在 variable.styl 文件中做了如下定义。 $main = #234235 $font-base = &quot;14px&quot; $font-medium = &quot;16px&quot;我们可以看到，对象的 key 和 value 分别作为了变量名和变量的值。如果有嵌套的多层对象，那么对象将被全部展开到最底层，每一层的 key 将以中划线的形式进行连接。这就是 json 函数的使用方法。 有了这个方法，我们就可以实现之前提到的目标，在同一个文件中管理 stylus 变量在 CSS 和 JS 中的使用。 stylus 还是有很多高级的功能可以使用的，只不过我们平时只是常用他最基础的那些功能而已。在看 json 这个函数的文档的时候，我向下翻了一下，看到了use 函数。这个函数的作用是可以自定义一些新的函数，让 stylus 编译器可以识别这些函数。这就给了我们开发者很大的自由度，让我们可以实现一些骚操作。具体的使用方式还是见文档吧。但是这个文档写的不太仔细，详细使用方法说的不是很确切，有时间我可以再研究一下。","path":"blog/uncategorized/stylus-变量管理/"},{"title":"canvas中的图片跨域问题","text":"canvas的图片跨域问题是指，当在canvas中使用和当前网站不同域名的图片资源的时候，如果你想使用canvas的toDataURL等转化成实际图片进行处理，比如下载。这时候你会发现跨域的图片的那部分将无法加载出来。 这种事情是非常常见的，因为我们很多情况下并不会把图片资源和网页挂在同一个域名下。当然，你也可以对某些子路径的域名做一层反代规避掉这个问题。如果你没有做这个，那么你应该怎么办呢？ 这时候你就需要做两件事： 首先在你存放文件的地方，如果是在腾讯云或者阿里云这种，直接设置图片的跨域头。也就是加上Access-Control-Allow-Origin: ${ your domain }的跨域头。当然如果你嫌麻烦，跨域直接把${ your domain }这一部分写成*，这样任何网页都可以使用你的图片。 然后在 img 标签上添加一个属性,有两种方法添加 通过js添加，也就是在那个img标签中做如下操作 const image = new Image() image.crossOrigin = &quot;anonymous&quot; image.src = &quot;url&quot; // set a true url 直接在HTML代码中添加 &lt;img src=&quot;url&quot; crossorigin=&quot;anonymous&quot; /&gt; 但是，以上的解决方法都是在图片的控制权在你的手上的情况下。如果这是一张第三方的图片，你就无法设置他的跨域头来解决问题了。就比如产品希望我在canvas上画上一个微信生成的二维码。我不是马化腾，没有能力让腾讯的工程师给我加一个跨域头。所以这种情况下，也有两个办法： 让后端的同学们帮忙开一个接口，下载那个图片。后端的接口是可以控制的。 通过nginx设置反代来获取那张图片，并且在nginx中加入设置跨域头的配置。","path":"blog/uncategorized/canvas中的图片跨域问题/"},{"title":"JS正则bug","text":"让我们来看一段代码 const reg = /\\d/g console.log(reg.test(&quot;1&quot;)) // true console.log(reg.test(&quot;1&quot;)) // false console.log(reg.test(&quot;1&quot;)) // true console.log(reg.test(&quot;1&quot;)) // false这时我们会发现，有两次正则的 test 方法得出的结果是错误的。这是什么原因呢？让我们再看几段代码。 const reg = /\\d/ console.log(reg.test(&quot;1&quot;)) // true console.log(reg.test(&quot;1&quot;)) // trueconsole.log(/\\d/g.test(&quot;1&quot;)) // true console.log(/\\d/g.test(&quot;1&quot;)) // true我们可以看到，我们换了两种写法，这个 bug 就不再出现了。一个是去掉了正则的 g 这个 flag。一个是不再使用定义的正则常量，而是每次都使用新的正则实例。显然，这个 bug 出现的原因就在与这个正则常量以及 g 这个 flag 身上。让我们再看一段代码。 const reg = /\\d/g console.log(reg.lastIndex) // 0 console.log(reg.test(&quot;1&quot;)) // true console.log(reg.lastIndex) // 1 console.log(reg.test(&quot;1&quot;)) // false console.log(reg.lastIndex) // 0 console.log(reg.test(&quot;1&quot;)) // true console.log(reg.lastIndex) // 1 console.log(reg.test(&quot;1&quot;)) // false console.log(reg.lastIndex) // 0我们可以看到，这个正则常量有一个叫做 lastIndex 的属性，这个属性是在正则使用了 g 这个 flag 的时候才会有作用。用处是记录上一次正则匹配的结果的索引值 +1，匹配失败则返回0。这样存在 g 这个 flag 的正则可以进行多次匹配。让我们来看一段代码来体会他的作用。 const reg = /\\d/g console.log(reg.exec(&quot;12&quot;)) // [&quot;1&quot;, index: 0, input: &quot;12&quot;, groups: undefined] console.log(reg.exec(&quot;12&quot;)) // [&quot;2&quot;, index: 1, input: &quot;12&quot;, groups: undefined] console.log(reg.exec(&quot;12&quot;)) // null在使用正则进行多次匹配的时候，就是 lastIndex 这个属性记录的上一次匹配的位置，并告诉你他下次开始匹配的位置的索引值。我们也可以修改 lastIndex 来改变。 const reg = /\\d/g console.log(reg.exec(&quot;12&quot;)) // [&quot;1&quot;, index: 0, input: &quot;12&quot;, groups: undefined] reg.lastIndex = 0 console.log(reg.exec(&quot;12&quot;)) // [&quot;1&quot;, index: 0, input: &quot;12&quot;, groups: undefined]可以看到，这时候又从头开始匹配了。在我们平时的业务代码中，有时候经常会需要一个类型的正则，就比如说检查输入的数字是否是合法的金额，并且在好几个地方都会用到这个正则。这种情况下我们可能就会定义一个常量来储存这个通用的正则。所以我们平时要小心这个问题。 但是，在我们的第一段代码中，显然这种结果不是我们所预期的。这显然是可以称为一个 bug 的。想要避开这个问题，一共有三种解决方法。 /\\d/g.test(&quot;1&quot;)第一种我认为是最好的，可以最优雅的解决这个问题。就是每次用正则的时候都使用新的实例，不要定义常量。 cosnt reg = /\\d/ reg.test(&quot;1&quot;)第二种就是不使用 g 这个 flag 就不会触发 lastIndex 的问题。但是有时候我们确实会需要 g 这个 flag，并不一定能完美的解决问题。 const reg = /\\d/g reg.test(&quot;1&quot;) reg.lastIndex = 0第三种就是每次使用过之后，都将 lastIndex 属性重置为 0。这个方法就不是非常优雅。","path":"blog/js/JS正则bug/"},{"title":"JS 异步陷阱","text":"今天在写代码的时候 linter 报了一个错，名字是 require-atomic-updates 的错误。具体的代码形势如下。 let _db = {} async function a() { _db.contract = (await axios.get(&quot;/api/contracts/info&quot;)).data }我乍一看，这种基础的代码还能有什么规范错误的吗？最后，我发现，其实 eslint 期望检查出的是这种类型的错误。 let a = 0 function setA(value) { a += (await axios.get(&quot;/api/contracts/info&quot;)).data }这种情况下，JS 会记住a的值，如果在这个异步过程中a的值被其他东西改变了，那么将得到错误的结果。 在 GitHub 上有一个关于这个问题的 issue： require-atomic-updates false positive","path":"blog/uncategorized/JS-异步陷阱/"},{"title":"SVG 详解","text":"什么是SVGSVG 是 Scalable Vector Graphics 的缩写。字面意思就是可伸缩矢量图形。他相对于传统的格式的图片（如png，jpg等）最大的优势就是他是可以无限缩放的。无论放大多少倍，他的图片都不会出现模糊的问题。而且他的兼容性也很好。除了 IE8 及以下，其他浏览器都对其提供了良好的支持。 SVG 语法SVG 使用 XML 来定义它的图像。常用的元素有： text: 创建一个文字元素 circle: 创建一个圆形 ellipse： 创建一个椭圆 rect: 创建一个矩形 line: 创建一条直线 path: 创建一个路径 textPath: 创建一个文本路径 polygon: 创建一个多边形 g: 创建一个元素组 text属性x，y代表文本绘制的起始点 &lt;text x=&quot;10&quot; y=&quot;10&quot;&gt;I am a text&lt;/text&gt;circle属性如下 cx: 圆心x坐标 cy：圆形y坐标 r：圆半径 fill：圆填充色 &lt;circle cx=&quot;20&quot; cy=&quot;20&quot; r=&quot;10&quot; fill=&quot;#e0e0e0&quot; /&gt;ellipse属性如下 cx： 椭圆圆心x坐标 cy： 椭圆圆心y坐标 rx： 椭圆横向半径 ry： 椭圆纵向半径 rect属性如下 x： 矩形左上角x坐标 y： 矩形左上角y坐标 width： 矩形横向长度 height：矩形纵向长度 line属性如下 x1: 直线起点x坐标 y1: 直线起点y坐标 x2: 直线终点x坐标 y2: 直线终点y坐标 path有一个属性d，在这个属性中使用一些命令来绘制图形，命令如下 M： 代表 moveto，接受一组x，y坐标，把绘制点移动到这个位置 L： 代表 lineto，接受一组x，y坐标，从上一点开始到这个参数的点绘制一条直线 H： 代表 horizontal lineto，接受一个x坐标，类似L，但是是绘制一条纵向的直线 V： 代表 vertical lineto，接受一个y坐标，类似L，但是是绘制一条横向的直线 C： 代表 curveto，接受三组x，y坐标，绘制一个三次贝塞尔曲线 S： 代表 smooth curveto，接受两组x，y坐标，一个点某一侧的控制点是它另一侧的控制点的对称（以保持斜率不变），是简写的C命令 Q： 代表 quadratic Bézier curve，接受两组x，y坐标，绘制一条二次贝塞尔曲线 T： 代表 smooth quadratic Bézier curveto，接受一组x，y坐标，类似S命令，是Q命令的简写，可以只写终点自动推测中间的控制点。但是之前必须有Q命令或者T命令，否则将会成为一条直线 A： 代表 elliptical Arc，A命令有许多参数，是用来生成一段曲线的 Z： 代表 closepath，不接受参数，代表将终点和起点相连，闭合路径 &lt;path d=&quot;M 10 10 L 20 20 H 30 V 40 Z&quot; /&gt;textPath接受的参数和path相同。然后文字将在这段路径上显示 &lt;textPath d=&quot;M 10 10 L 20 20 H 30 V 40 Z&quot;&gt;i am a text path&lt;/textPath&gt;polygon有一个属性 points，接受多个x，y坐标组 &lt;polygon points=&quot;10,10 20,20 35,20&quot; /&gt;g将多个元素分组 &lt;g id=&quot;group&quot;&gt; &lt;path d=&quot;M10 10 L20 20&quot; /&gt; &lt;rect x=&quot;20&quot; y=&quot;20&quot; width=&quot;10&quot; height=&quot;10&quot;&gt; &lt;/g&gt;SVG 常用属性 fill: SVG 的填充颜色。在改变svg颜色的时候不能用color，而是要用fill。当然，你可以将fill属性设为currentColor，那么 SVG 将使用 color 属性的颜色进行渲染 fill-opacity： 填充颜色的不透明度 stroke: SVG 线条的颜色 stroke-width: SVG 线条的宽度 使用JS，CSS操作SVG当 SVG 使用内联的方式加载时（不是通过img等标签通过外部引入），我们可以通过JS或者CSS来操作SVG。SVG 中的每一个标签和正常的DOM标签一样，都拥有class，id等属性。可以通过设置CSS来改变样式，同时，他们也可以被类似于document.querySelector 等操作DOM的API来进行改变。通过这样，我们可以实现一些很炫酷的SVG动画等等功能。 viewport 和 viewBoxviewport是指 SVG 可见区域，也就是 SVG 实际渲染的大小 &lt;svg width=&quot;200&quot; height=&quot;200&quot;&gt;&lt;/svg&gt;这里定义了 SVG 长宽为 200px，这个200px的大小就是 SVG 的 viewport viewBoxviewBox 是指渲染的画布的区域范围 viewBox 接受4个参数：x, y, width, height 定义了一个矩形。举一个例子 &lt;svg viewBox=&quot;0,0,50,50&quot;&gt; &lt;rect x=&quot;10&quot; y=&quot;10&quot; width=&quot;10&quot; height=&quot;10&quot;&gt; &lt;/svg&gt;这个 SVG 渲染出来的结果是什么呢？我们假设这个 SVG 设置长宽为100。那么展示的矩形起点在20，20处，长宽为20,20。 也就是说，我们想象 SVG 有一个无限大的画布，里面的元素可以在这个任意大的画布中进行作画。但是我这个 SVG 向外界展示的区域是一个有限的区域，我们对 SVG 缩放的范围，渲染的范围就是这个区域。这个区域的名字就是 viewBox。 preserveAspectRatio但是我们设置的 SVG 的长宽比和 viewBox 的长宽比并不一定是一样的。这时候就需要 preserveAspectRatio 属性来出马了。他由两个值组成，第一个值表示 viewBox 和 viewport 的对其方式，第二个值表示长宽比如何维持。 第一个值有10种 一种是none，这时候 SVG 的长宽比将失真，强制使 viewBox 长宽适应 viewport 的长宽 另外的9种，他们分别定义了x，y方向上，以起点，终点和中点三个方向进行对其，3*3 共 9 种 第二个值： meet： 保持 viewBox 纵横比缩放， viewBox 尽可能放大占满 viewport。也就是说，整个整个 viewBox 区域都是可见的，小于 viewport 的大小 slice： 保持 viewBox 纵横比缩放，同时比例小的方向放大填满 viewport。也就是说，部分 viewBox 区域可能会超出 viewport 的区域变得不可见","path":"blog/HTML/SVG-详解/"},{"title":"Hugo with Vue","text":"本文主要讲述如何在 Hugo 中使用 Vue 进行开发一些复杂交互的网站。 HugoHugo 是一个 static site generator(静态网站生成)的框架。静态网站生成指的是利用一些模板文件，预先将所有的HTML文件生成好，挂载在服务端的一种技术。利用 Hugo 你就可以生成一系列网页。但是由于这种框架都是只基于简单的 HTML，JS，CSS，而不涉及任何其他的框架，所以如果要开发一些比较复杂的网站的时候就会变得比较困难。这时候，利用一些现代的框架（如Vue，React，Angular等等）来简化我们的开发。这里我们就以 Vue 为例来看看怎么使用 Vue 来配合 Hugo 进行开发。 前提知识本文将假设读者已经了解以下知识： Hugo 的基本知识（文件夹结构，编译流程等等） Vue 的基本知识 webpack 配置基础 基本思路想要使用Vue进行开发，就先思考一下，我们平时使用的Vue编译后的结果是什么样子的。只要我们模仿一下这个编译后的结果，就可以使用Vue进行开发了。 首先，我们肯定是要分两种环境进行考虑的，一种是开发环境，一种是生产环境。开发环境使用热加载开发简便。生产环境直接编译出结果可以使用。 &lt;body&gt; &lt;script src=&quot;localhost:8080/app.js&quot;&gt;&lt;/script&gt; &lt;/body&gt;以上大致就是开发环境中Vue的关键代码。所以在开发环境中，我们的想法是，利用webpack-dev-server启动Vue部分的代码，hugo部分的代码中判断环境，如果是开发环境就插入&lt;script src=&quot;localhost:8080/app.js&quot;&gt;&lt;/script&gt;这段代码。 &lt;body&gt; &lt;script src=&quot;.../vender.vue.[hash].js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;.../vendor.[dep].[hash].js&quot;&gt;&lt;/script&gt; &lt;/body&gt;以上大致就是生产环境中Vue的关键代码。也就是很多的js业务代码，运行时的依赖文件，css文件等，要把这些依赖引入到文件之中。所以在生产环境中，我们的想法是，利用HTML-webpack-plugin把我们所有的依赖的引入代码打包成一个文件，然后在hugo中判断是生产环境就引入并序列化这个文件的内容（hugo 有一个叫 readFile 的函数可以读取一个文件内容并序列化到模板文件中）。 文件结构设计该项目的文件结构如下 root(hugo root folder) - content - layouts - ...(template files) - ui(Vue root folder) package.json其中，在 template files 中代码的末尾加入如下代码。 &lt;div id=&quot;root&quot;&gt;&lt;/div&gt; {{ if eq (getenv \"HUGO_DEV\") \"true\" }} &lt;script src=&quot;http://localhost:8800/app.js&quot;&gt;&lt;/script&gt; {{ else }} {{ readFile \"/ui/dist/body.partial\" | safeHTML }} {{ end }}这样，在生成的模板文件中，会根据环境来判断引入的script标签。在开发环境使用webpack-dev-server生成的app.js文件。在生产环境中，利用HTML-webpack-plugin生成的文件，把项目需要是引入的文件全部导入。 最后，在ui文件夹下就可以看做一个完整的Vue的项目。配置webpack分环境进行打包。开发环境下起webpack-dev-server，生产环境下打包进dist目录。具体的webpack配置后文将细说。 数据获取方式由于要用Vue来进行网页的编码，那么我们就不可能像使用hugo一样直接在模板文件中写HTML，而是要利用Vue来生成不同的页面。要生成不同的页面，其关键点就在于同类型的页面要获取不同的数据。所以说，我们的做法是预先把数据存储在网页之中。 我们获取的数据源一共有两个，一个是定义在data文件夹中，一个是定义在content文件夹中md文件的frontmatter中。我们可以在模板文件中，将这些数据定义在window下，作为全局变量。这样就可以在Vue中进行调用这些数据。 { window.data = &quot;{{ Site.Data.config }}&quot; window.title = &quot;{{ .Title }}&quot; }还有一部分数据是定义在md文件中的markdown。这一部分也可以将其挂载在模板文件中，但是使用 display:none 进行隐藏。然后再使用Vue操作这一部分的DOM即可。 &lt;div id=&quot;_page-content&quot; style=&quot;display: none&quot;&gt; {{ .Content | markdownify }} &lt;/div&gt;打包方式我们将打包的命令配置在package.json文件中。具体的命令定义如下。 &quot;scripts&quot;: { &quot;hugoDev&quot;: &quot;cross-env HUGO_DEV=true hugo server --bind 0.0.0.0 --noHTTPCache --disableFastRender&quot;, &quot;webpackDev&quot;: &quot;cd ui &amp;&amp; node ../node_modules/webpack-dev-server/bin/webpack-dev-server.js&quot;, &quot;hugoBuild&quot;: &quot;hugo -d dist&quot;, &quot;webpackBuild&quot;: &quot;cd ui &amp;&amp; rm -rf dist &amp;&amp; cross-env BUILD=true node ../node_modules/webpack/bin/webpack.js --prod --progress --hide-modules&quot;, &quot;assetCopy&quot;: &quot;cp -r ui/dist dist/assets&quot;, &quot;dev&quot;: &quot;npm-run-all --parallel hugoDev webpackDev&quot;, &quot;clean&quot;: &quot;rm -rf dist&quot;, &quot;build&quot;: &quot;yarn clean &amp;&amp; yarn webpackBuild &amp;&amp; yarn hugoBuild &amp;&amp; yarn assetCopy&quot; }开发环境使用的命令是yarn dev。我们看到这里使用了npm-run-all插件。因为在开发环境webpack-dev-server和hugo的任务并不会结束，所以直接使用&amp;&amp;符号进行串行的任务并不能执行两个命令，所以要利用他让我们可以一个命令同时执行hugo和webpack的命令。我们可以看到，这个命令同时执行了两个命令。第一个 hugoDev 就是在启动开发环境hugo的编译流程，我们可以注意到其中一开始有一段cross-env HUGO_DEV=true的代码就是为了给 template files 中的代码区分环境用的。我们回顾一下在上一节中提到，可以发现插入在末尾的代码中判断环境使用的变量就是 HUGO_DEV。第二个命令就是启动webpack-dev-server。这样开发环境就可以正常使用了。 生产环境使用的命令是yarn build。这个编译流程分为了四步。第一个命令yarn clean用于清除上一次编译留下来的文件。第二个命令yarn webpackBuild是执行webpack的打包流程，把Vue的运行时代码打包成一个文件夹。第三个命令yarn hugoBuild是执行hugo的打包流程。最后一个命令yarn assetCopy把webpack打包出的代码复制到hugo打包出的文件夹下，生成一个最终的项目打包代码文件夹。 webpack 配置webpack 配置文件中的大部分都与普通的Vue项目没有什么区别，唯一有区别的地方在于 HTML-webpack-plugin 的配置。 首先，在开发环境，并不需要做任何特殊的配置。编译输出了一个app.js的文件给webpack-dev-server，直接就可以使用了。 在生产环境的编译下，对配置做如下修改。 new HTMLWebpackPlugin({ inject: false, minify: false, filename: &quot;head.partial&quot;, templateContent: ({htmlWebpackPlugin}) =&gt; `${htmlWebpackPlugin.tags.headTags}`, }) new HTMLWebpackPlugin({ inject: false, minify: false, filename: &quot;body.partial&quot;, templateContent: ({htmlWebpackPlugin}) =&gt; `${htmlWebpackPlugin.tags.bodyTags}`, })加入了上述两个HTML-webpack-plugin的配置后，这两者将分别把JS文件和CSS文件的引入标签代码打包成了 body.partial 和 head.partial 两个文件。让我们来看看这两个文件。 // body.partial &lt;script src=&quot;/assets/js/vendor.runtime.bf9a7ce533402d6e1e4b.js&quot;&gt;&lt;/script&gt;&lt;script src=&quot;/assets/js/vendor.vue.10426c5f37b9909f3356.chunk.js&quot;&gt;&lt;/script&gt;&lt;script src=&quot;/assets/js/vendor.core-js.0dd09ca0b957f339ec22.chunk.js&quot;&gt;&lt;/script&gt;&lt;script src=&quot;/assets/js/vendor.normalize.css.c25d28ba90daebf55ddd.chunk.js&quot;&gt;&lt;/script&gt;&lt;script src=&quot;/assets/js/vendor.velocity-animate.96372836ed735c252945.chunk.js&quot;&gt;&lt;/script&gt;&lt;script src=&quot;/assets/js/app.c1b601945336515e1a70.chunk.js&quot;&gt;&lt;/script&gt;// head.partial &lt;link href=&quot;/assets/css/vendor.normalize.css.24bf1742e3764eb5de3c.chunk.css&quot; rel=&quot;stylesheet&quot;&gt;&lt;link href=&quot;/assets/css/app.fd7d6ea5c66bbd36de53.chunk.css&quot; rel=&quot;stylesheet&quot;&gt;然后上述的两个文件在生产环境下分别被以下的代码引入模板文件。 {{ if not (eq (getenv \"HUGO_DEV\") \"true\") }} {{ readFile \"/ui/dist/head.partial\" | safeHTML }} {{ end }} {{ if eq (getenv \"HUGO_DEV\") \"true\" }} &lt;script src=&quot;http://localhost:8800/app.js&quot;&gt;&lt;/script&gt; {{ else }} {{ readFile \"/ui/dist/body.partial\" | safeHTML }} {{ end }}这样就完成了一个 Hugo with Vue 项目的配置。","path":"blog/HTML/Hugo-with-Vue/"},{"title":"Unicode and utf-8","text":"ASCII我们知道，最一开始，我们计算机使用 ASCII 作为存贮字符集的一种规范。使用八个 bit，来表示一个字符。8 bit 一共有256中组合可能。他们使用了其中128种作为存储英文字符的编码规范。他们称之为ANSI 的”Ascii”编码（American Standard Code for Information Interchange，美国信息互换标准代码）。 GBK在计算机来到中国后，当时的字符集还只有 ASCII，所以为了可以把中文编码，就创造了 GB2312 标准编码。使用两个字节来表示一个汉字。当两个大于127的字节连在一起时，就认为使用了 GBK。这就是我们常说的全角字符。而127以下的则是半角字符。但是两个127以上的字节只能表示7k多个字符，所以后来我们扩展了字符集，不再要求低位小于127，由此把更多的汉字和其他少数民族等的字符加入进去，扩展成了 GB18030。 UNICODE规定了 128 个字符的字符编码。对于英文来说，这128个够用了，但是对于全世界的语言来说，明显这一个字节的空间就不够用了。所以说世界各地为了应对这种情况创造出了很多种不同的编码标准。这无疑造成了很多麻烦。为了统一全世界文字的编码方式，我们就制定了 Unicode 这个标准。使用更多的字节来存储这些字符。 UTF-8虽然 Unicode 解决了统一字符存储标准的问题，但是 Unicode 为了存储更多的字符而使用更多的存储空间显然造成了极大空间的浪费。这是不可接受的。所以，为了更好的使用 Unicode，UTF-8 出现了。UTF-8 是 Unicode 的一种实现方式。 UTF-8 是一种可变长的编码方式。他可以使用1-4个字节来表示一个符号。UTF-8 的编码规则有两条： 单字节的字符，字节的第一位设为0，对于英语文本，UTF-8码只占用一个字节，和ASCII码完全相同 n个字节的字符(n&gt;1)，第一个字节的前n位设为1，第n+1位设为0，后面字节的前两位都设为10，这n个字节的其余空位填充该字符unicode码，高位用0补足。 0xxxxxxx 110xxxxx 10xxxxxx 1110xxxx 10xxxxxx 10xxxxxx 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx举一个例子，我 这个汉字的 Unicode 为 %u6211，用二进制表示为 0110 0010 0001 0001。可以看到，我们需要15位来存储这个汉字。让我们看看上面那串转换规则。一个x就表示一位可以存储的空间。一个字节可以表示7位，不够。两个字节可以存储11位，还是不够。三个字节可以存储16位，够了。所以说我们使用三个字节来存储这个字符。让我们开始填充。 0110 0010 0001 0001 变成 110 001000 010001 1110xxxx 10xxxxxx 10xxxxxx 填充 11100110 10001000 10010001 也就是 E68891我们查一下 我 这个汉字的 UTF-8 编码。正是 \\xE6\\x88\\x91。","path":"blog/code/Unicode-and-utf-8/"},{"title":"CSS Variables","text":"什么是 CSS Variables显而易见，这是一个让你可以在 CSS 中使用变量的技术。 兼容性除了 IE 以外，所有主流的浏览器都对 CSS Variables 有很好的支持。但是你可以引入 polyfill，css-vars-ponyfill 将让你可以在 IE9+ 的浏览器中使用 CSS Variables。 语法CSS Variables 的语法非常简单。定义变量一定以双中划线开头，使用 var 函数来使用定义过的变量。var 函数的第二个参数支持默认值。当那个变量没有定义的时候将使用默认值。var 函数将第一个逗号后的所有参数视为默认值，不会处理其中的逗号，引号等等。 // define --gray: #F5F5F5 // use color: var(--gray) // use with default value color: var(--gray, #E0E0E0) border: var(--border, 1px solid gray)如果定义的是一个数字，后面不可以直接跟单位，但是你可以使用 calc 函数来做这件事 --twenty: 20 // wrong height: var(--twenty)px // right height: calc(var(--twenty) * 1px)作用域CSS Variables 和正常的 CSS 一样也有作用域的规则。权重更高的变量将生效。一般情况下，我们将CSS Variables 定义在 :root 的 CSS 选择器中。这样所有的地方都可以使用这些变量。 JS 与 CSS 交互我们可以使用 JS 设置一些 CSS Variables，也可以通过 JS 读取 CSS Variables。 // set document.body.style.setProperty(&quot;--gray&quot;, &quot;#E0E0E0&quot;) // get document.body.style.getPropertyValue(&quot;--gray&quot;) // delete document.body.style.removeProperty(&quot;--gray&quot;)作用定制主题我们有一个常见的需求，就是网页除了正常的样式外，还要定制一个暗黑模式。这种情况下，CSS Variables 就是一种实现的方案。我们只需要定义两套不同的 CSS Variables，分别定义在暗黑模式和正常模式下的调色板是什么。通过程序控制加载什么变量即可。 CSS 存储信息我们可以看到，我们现在可以通过 JS 来读取 CSS Variables，所以我们可以在 CSS 中存储一些信息然后通过 JS 读取。 JS 设置 CSS比如说我们要实现一个进度条。那么我们可以通过使用 CSS Variables 的方案，设置他的宽度为一个CSS Variable，然后通过用 JS 改变 CSS Variable 的值来改变进度条的进度。","path":"blog/CSS/CSS-Variables/"},{"title":"什么是AST","text":"AST 是 Abstract Syntax Tree（抽象语法树）的缩写。简单的来说就是用来解析你写的代码结构的结果。 AST 的作用是非常巨大的。我们平时最常用的 babel，webpack 等等生产力工具都会用到 AST 这个东西。 现在，我们就来简单的看一看一个实际的 AST 的例子吧。 function add(a, b) { return a + b }现在我们写了一个简单的 add 函数。希望将他解析成一个 AST。 现在存在很多种解析的引擎 esprima acron Traceur UglifyJS2 shift … 我们选择其中的一种进行解析。 const esprima = require(&quot;esprima&quot;) const code = ` function add(a, b) { return a + b } ` const ast = esprima.parseScript(code) console.log(JSON.stringify(ast, null, 2))将以上的代码进行解析，转化为 AST 的结果是 { &quot;type&quot;: &quot;Program&quot;, &quot;body&quot;: [ { &quot;type&quot;: &quot;FunctionDeclaration&quot;, &quot;id&quot;: { &quot;type&quot;: &quot;Identifier&quot;, &quot;name&quot;: &quot;add&quot; }, &quot;params&quot;: [ { &quot;type&quot;: &quot;Identifier&quot;, &quot;name&quot;: &quot;a&quot; }, { &quot;type&quot;: &quot;Identifier&quot;, &quot;name&quot;: &quot;b&quot; } ], &quot;body&quot;: { &quot;type&quot;: &quot;BlockStatement&quot;, &quot;body&quot;: [ { &quot;type&quot;: &quot;ReturnStatement&quot;, &quot;argument&quot;: { &quot;type&quot;: &quot;BinaryExpression&quot;, &quot;operator&quot;: &quot;+&quot;, &quot;left&quot;: { &quot;type&quot;: &quot;Identifier&quot;, &quot;name&quot;: &quot;a&quot; }, &quot;right&quot;: { &quot;type&quot;: &quot;Identifier&quot;, &quot;name&quot;: &quot;b&quot; } } } ] }, &quot;generator&quot;: false, &quot;expression&quot;: false, &quot;async&quot;: false } ], &quot;sourceType&quot;: &quot;script&quot; }我们可以通过上面这个 json 来很好的描述出 add 函数的代码。 在我们的日常的生产力工具中，AST 占据了不小的地位。无论是 uglify，es6 到 es5 的转化，只要是对于我们代码结构解析的都无法离开 AST 的存在。这是因为 AST 不止可以解析代码结构，同样的也可以操作代码结构并生成修改后的代码结构。解析 -&gt; 操作 -&gt; 生成 这三步，正是这些生产力工具的基本原理。","path":"blog/js/什么是AST/"},{"title":"几种CSS管理方案","text":"CSS 的管理一直都是一个让人头大的难题。在 SPA 中，由于 CSS 的作用域是全局的，所以每一个页面的文件的样式都可能会影响到其他页面的样式。当你无意识中写了一个和其他页面同名的样式时，这无疑是致命的。而当你在对 DOM 结构进行删改时，非常容易遗留下无用的 CSS。所以如何合理的维护一个工程的 CSS 无疑是一个巨大的难题。 BEMBEM 是最简单的解决 CSS 命名冲突的方案。是一种通过约定来解决命名冲突的实现方式。 .block {} .block__element {} .block__another-element {} .block--modifier {} block 代表了一个命名空间。可能是一个页面，可能是一个组件，也可能是一个组件中的一部分。可以自行约定。 所有在 block 命名空间下的样式都已 block 开头，并已双下划线连接自己的名字 中划线作为连字符 双中划线连接的代表不同的状态 不可嵌套过深的层级，避免出现过长的 CSS 当然这些都是一些约定，所以自然你也可以加入一些自己的约定。 BEM 结构清晰，易于扩展，但是很容易出现 CSS 类名过长，CSS 文件过于繁琐的问题。 CSS in JS显而易见的，CSS in JSS 就是在 JS 代码中写 CSS。具体是什么样子的，我们就来看一个最流行的实现 CSS in JS 的库的代码。 import React, { Component } from &quot;react&quot; import styled from &quot;styled-components&quot; const Wrapper = styled.div` background: black ` const Title = styled.h1` background: white ` class App extends Component { render() { return ( &lt;Wrapper&gt; &lt;Title&gt;Hello world!&lt;/Title&gt; &lt;/Wrapper&gt; ) } } export default App这一份代码应该十分的浅显易懂。每一个标签都由 styled 使用 ES6 的新语法，变迁模板字符串生成一个 HOC。这样每一层样式都定义成了一个组件来进行使用。 他的实现方式并不是简单的使用内联样式，而是生成了一个随机的字符串作为类的名字，并且保证了每一个类名都不会重复。这样就完美的解决了之前提到的样式名称重复的问题。 优点： 可以在 CSS 中使用 JS 的变量 可以在 CSS 中使用 JS 来进行计算 解决了全局命名冲突的问题 对于推崇 everything in JS 理念的人很友好 缺点 不能使用 post CSS 不能使用 CSS 预处理器 难以调试，在 devtool 中只能修改当前的组件实例 性能不可避免的更低 生成的代码有很多冗余 将样式和 JS 代码混合在一起，对一些人来说不可接受 除此之外，前面提到的优点都不是没有解决方法的，比如使用 BEM，使用 less，sass 等等。 CSS ModuleCSS Module 的开启方法是只要在 webpack 的 css-loader 配置中的 options 选项中把 module 设为 true 即为开启了 Css Module。 让我们看看 CSS Module 的使用方式。 // index.js import Index from &#39;./index.css&#39; const html = `&lt;div class=${ Index.header }&gt; &lt;h1 class=${ Index.title }&gt;CSS Modules&lt;/h1&gt; &lt;h1 class=&quot;title&quot;&gt;global class title&lt;/h1&gt; &lt;/div&gt;`// index.css .header { background: black; } .title { color: white; } :global(.title) { composes: header; color: green; }CSS Module 的作用相当于把每一份 CSS 文件的进行了隔离。然后可以 import 进来引用他的名字。当不想只有页面的作用于的类在前面加上 global 即可。这样可以直接在使用他本身的类名而不用 使用 import 进来的名字。可以使用 composes 来复用已存在的类。 他的原理十分的简单，就是 webpack 在编译的时候用哈希的方式另外生成了一套类名。而 import 进来的就是原来的类名和编译后的类名的映射的对象。","path":"blog/CSS/几种CSS管理方案/"},{"title":".git 文件夹探秘","text":"什么是 .git 文件夹.git 文件夹是 git 存储整个仓库的信息的位置。如果想要知道整个仓库的信息，那么只要这个文件夹就足够了。这个文件夹的结构如下所示。 文件夹 - hooks - info - logs - refs - objects 文件 - config - description - FETCH_HEAD - HEAD - index - ORIG_HEAD - packed-refs接下来我们来一个个看看这些文件夹都是用来干什么的。理解了这些文件夹是干什么的之后想必你对于 git 的理解将会加深一个层次。 在解释 这个文件夹结构之前，我们首先要知道一个基础的知识。那就是 git 和以前的 SVN 这种版本控制系统的差别在哪里。 SVN 这种版本控制系统的原理是每一个改变都会作为一个 diff 记录下来。当前的版本就是所有 diff 之和。而 git 的存储方式是每一个文件的每一个版本都会作为一个快照保存下来。一次 commit 就相当于记录了所有文件的快照。 hookshooks 文件夹存储的是一些脚本，可以在不同的阶段运行，比如 commit、pull 等等。 infoinfo 文件夹中有一个文件名字叫 exclude。这个文件是派什么用的呢？顾名思义，其实这个文件的作用和 .gitignore 的作用是一样的，不过这个文件每一个人是可以不一样的，也就是说一个团队的每个人可以有一份自己的 .gitignore 文件。 logs顾名思义，这个文件夹的作用就是日志，给你反悔用的。我们看看这个文件夹下的文件结构 文件夹 - refs 文件 - HEAD我们打开 HEAD 文件看一下，可以发现是如下的信息。 ...... 9bb7f56e59fb83ea7b7c55a23409f8a2ab64a033 25c9b1108072c99c0643bad5d49092861f14e492 yifeidai &lt;floatdai@gmail.com&gt; 1581662743 +0800 commit: BBS 3114 25c9b1108072c99c0643bad5d49092861f14e492 ce476df4e0c091b1ee00c0009778ca0fa8871c1f yifeidai &lt;floatdai@gmail.com&gt; 1581663490 +0800 commit: BBS 3120 ce476df4e0c091b1ee00c0009778ca0fa8871c1f 1f1295bda02bb5d7e19b66bf3c86264bfdc0ce62 yifeidai &lt;floatdai@gmail.com&gt; 1582288127 +0800 commit: Finish sprint 20 1f1295bda02bb5d7e19b66bf3c86264bfdc0ce62 a6d8378f0fcf66c6f00b61f5ce7150d645727c28 yifeidai &lt;floatdai@gmail.com&gt; 1588214020 +0800 pull: Fast-forward a6d8378f0fcf66c6f00b61f5ce7150d645727c28 978be520018e6a72359148afe2e5af9f5ba893a4 yifeidai &lt;floatdai@gmail.com&gt; 1588227237 +0800 commit: Add export data in welfare orders 978be520018e6a72359148afe2e5af9f5ba893a4 c1348b61a2dd28ce03a98e1327909b5dc0ba377d yifeidai &lt;floatdai@gmail.com&gt; 1588229105 +0800 commit: Add import data in welfare orders c1348b61a2dd28ce03a98e1327909b5dc0ba377d 572ecb8ccd0b4259cd2acabbec57445b682d74dd yifeidai &lt;floatdai@gmail.com&gt; 1588242537 +0800 commit: Add edit in welfare goods 572ecb8ccd0b4259cd2acabbec57445b682d74dd 401b1dff157a06b4dc53cf281bfbdb1c231128ae yifeidai &lt;floatdai@gmail.com&gt; 1588245717 +0800 commit: Fix welfare permission bug 401b1dff157a06b4dc53cf281bfbdb1c231128ae 462579259947c59c562df4c43152ef4440e2d3e7 yifeidai &lt;floatdai@gmail.com&gt; 1589969280 +0800 pull: Fast-forward 462579259947c59c562df4c43152ef4440e2d3e7 48b2c8aa20317ba0ee997897cd4b4c3a5ce47460 yifeidai &lt;floatdai@gmail.com&gt; 1589969439 +0800 commit: Change auth of management/userinfo可以看到，这是由一个个 commit 的信息组成的。然后再看看文件的名字： HEAD，显然这个就是 git 的 HEAD 变更的日志呀。再看看 refs 文件夹下的内容。 - heads - master - remotes - origin - HEAD - master - release显然这个就是一个本地和远程的每一个分支的文件组成结构。打开后可以发现他们的内容和之前的 HEAD 文件是相同的类型。也就是说他们分别记录了本地和远程每一个分支的变动信息。 记得如果我们有时候如果用了 hard 的 git reset 命令时想要找回被自己删掉的代码该怎么办吗？就是使用 git reflog 命令，会展示出所有的提交的信息。这些信息的来源就是这个文件夹。 refs 和 objectsrefs 文件夹和 objects 文件夹必须放在一起说明。我们之前说的那些文件夹其实都没有涉及到 git 的核心，也就是 git 的版本控制是怎么实现的。这其中的秘密就在这两个文件夹里面。 首先看一下 refs 文件夹下的文件结构 - heads - master - remotes - origin - HEAD - tags - v1.0.0 - v1.0.1 ...和上面 logs 文件夹的文件结构类似，这个文件结构包含了本地和远程所有的分支信息，除此之外还加上了 tags 的信息。那么显然每一个文件都是用来记载一个分支或者 tag 的详细信息的。那么这些文件是怎么记录这些信息的呢？让我们打开其中一个文件来看看。 heads/master 文件 48b2c8aa20317ba0ee997897cd4b4c3a5ce47460这个是什么？这个显然是一个 git 的 commit 的 SHA-1 啊。我们知道 git 的每一个提交都会生成一个 SHA-1 来作为这个 commit 的唯一标记。所以我们可以知道 refs 文件夹的作用是保存了每个分支和 tag 的当前的 commit 是什么的信息。但是这个文件夹并没有保存每个 commit 具体内容是什么，也没有保存整条分支，而是只知道一个分支的最新的提交是什么。那么这个任务也就是有 objects 文件夹来完成的。那么久让我们看看 objects 文件夹中有哪些东西。 - 0a - 0b - 0c - 0d - 0e ... - d0 - d4 - d5 - d6 ... - fc - fd - fe - info - pack中间省略号部分省略了很多两个字母的文件夹。哇，这些乱七八糟的文件夹到底是干什么的？很显然，这些都是两位的十六进制数组成的。再联想到我们之前 refs 文件夹中记录的内容 SHA-1，显然这是在为 SHA-1 做索引啊。我们打开 0a 这个文件夹，发现里面有两个文件。 9bbb2c1faa43e0dcb27e93ec9263ec4a5cc539 f23568e3ccab0fcfd4ce5e561076660e674d4c把这两个文件的前面加上 0a，可以发现这就是两个 SHA-1。也就说，之前在 refs 文件夹中记录的信息的意义找到了。这个 SHA-1 将这个 commit 的信息保存在了 objects 文件夹的一个个文件之中（注意，并不是所有的在 objects 文件夹中的文件都是这个，接下来我会意义说明）。那么就让我们打开之前之前记录在 heads/master 文件中的那串 SHA-1 的文件。由于 git 会对 objects 中的每一个文件做压缩，所以直接打开文件看到的都是乱码。 git 提供了指令 git cat-file 来查看 objects 文件夹中每一个文件的内容。那么就让我们输入 git cat-file -p 48b2c8aa20317ba0ee997897cd4b4c3a5ce47460 来看看这个文件中到底记录了什么。 tree bde4c5ed97de8cf3a311e8edc93ec2c1bbe7224e parent 462579259947c59c562df4c43152ef4440e2d3e7 author yifeidai &lt;floatdai@gmail.com&gt; 1589969439 +0800 committer yifeidai &lt;floatdai@gmail.com&gt; 1589969439 +0800我们先跳过第一行，看第二行。parent 代表的是这个 commit 的上一个 commit 的内容是什么，这个 commit 的对应的文件同样也可以在 objects 文件夹中找到。也就是说每一个 commit 的具体信息中都记录了上一个 commit 到底是什么。所以说之前的疑惑就得到了解决。在 refs 文件夹中的没有个分支文件都只记录了当前分支最新的 commit 是什么，那么怎么才能得出整个分支是什么样子的呢。那就是靠每一个 commit 都记录了他上一个 commit 是什么。一个个串起来就得到了一整条分支是有哪些 commit 构成的。 接下来看第三第四行，他们分别具体记录了作者和提交者的信息和提交的具体时间。 最后我们来看最重要的第一行。之前的那些信息让我们可以理解了 git 的 commit 树的信息是怎么存储的。但是，具体每一个 commit 他所包含的文件内容是什么还是没有。这个信息就是这个 tree 对象所存储的。tree 的意思其实就是相当于一个文件夹，最顶层的文件夹的信息当然就是一个提交的目录信息了。我们看看 tree 右边是什么？又是 SHA-1，说明这个 tree 的具体信息也是作为一个文件存储在 objects 文件夹之中的。说到这里大家可能有点蒙，objects 文件夹中村的不是所有 commit 的 SHA-1 吗？怎么突然有蹦出来一个具体 commit 的 tree 的 SHA-1？其实他们都是存储在 objects 文件夹之中的。这个文件夹存储了很多信息。可以看成一个索引的系统。那么就让我们看看这个 tree 中的信息是什么？同样的，我们使用 git log -p bde4c5ed97de8cf3a311e8edc93ec2c1bbe7224e 来查看这个文件 100644 blob cd68a1b91fea8d5f57f6bafe03b81bc0c89a78b0 .browserslistrc 100644 blob 107ed74e6bfd14740b6334a931034b99c542cb5f .gitignore 040000 tree 0274a81f8ee9ca3669295dc40f510bd2021d0043 .vscode 100644 blob 36109a8ca1e99df196b62750beffd2fb95ffd1a5 README.md 100644 blob 12da593be7f40fed6e53780801e3ad03d54affa4 babel.config.js 040000 tree d810c104ef132cd9765c070e9ee0f9a32db2d309 build 100644 blob a4dbc245f2bc6c829ea3d7ebc1b9d25f5d664cd7 index.html 100644 blob d9745d0b909f1a2497696f50bfd1347f399ca32b package.json 040000 tree 7c94cf2b4494855cd0b4f92d4a85cbfe36bf8e6b src 040000 tree 827cbc09e4a73bafe37c80eefd43f53f936cc89e static 100644 blob cead9117d91f1abc3339be2dcb0f7b71e70ce633 yarn.lock我们可以看到，这个文件中的第二列有两种值，blob 和 tree，之前说过了，tree 就相当于一个文件夹，那么 blob 显然就相当于一个具体的文件。我们再看看最后一列，他们就是每一个文件或文件夹的名字。而第一列则记录了这个文件或文件夹的具体类型等信息。和 linux 文件系统是不是很像。而第三列显然就是这些文件或文件夹的 SHA-1，他们具体的详细信息也将保存在另一个在 objects 文件夹中的文件中。tree 的文件保存的信息都和这个文件差不多，那么让我们看看 blob 类型的文件是什么样子的。同样的我们利用 git cat-file -p cd68a1b91fea8d5f57f6bafe03b81bc0c89a78b0 命令来看看 .browserslistrc 文件的内容是怎么保存的。 last 2 Chrome versions这个就是这个文件中的具体内容了。 这样一来整个 git 的存储逻辑就非常清晰了。首先在 refs 文件夹中对每一个分支都简建立一个具体的文件。这个文件中保存了每个分支最新的 commit 的信息。接下来每一个 commit 的信息都记录在 objects 文件夹中。首先，每一个 commit 的文件中记录的是提交者的信息。其次是该 commit 的上一个 commit 的信息，这让每一个 commit 都连了起来，记录一个 commit 就相当于记录了整个分支的 commit 信息。最后是 tree，也就是一个文件夹的信息。每一个文件夹的信息文件中都记录了下一层文件夹中的文件和文件夹信息。这就构成了每一次提交的文件结构快照。最后 tree 的末端，也就是文件夹的末端就是 blob 类型，一个个具体的文件。每一个文件的信息也作为一个个单独的快照文件保存在了 objects 文件夹之中。这结合起来就构成了每一个提交的具体内容快照。 config显然，这个文件夹就是我们每一个项目自己的 config， git config --local 命令中具体的信息就存储在这个文件中 description这是一个被 git web 所使用的文件，展示了这个仓库的描述。 FETCH_HEAD这个文件保存了 FETCH_HEAD 的引用是什么 HEAD打开这个文件，内容如下 ref: refs/heads/master这个文件显然记录了本地的 head 的具体信息。 index顾名思义，这个文件保存了 git 的 index 区（暂存区）的信息。这个文件也是经过加密的。我们可以通过 xxd -b -c 4 .git/index 命令看看这个文件的全貌。 00000000: 01000100 01001001 01010010 01000011 DIRC 00000004: 00000000 00000000 00000000 00000010 .... 00000008: 00000000 00000000 00000000 00000001 .... 0000000c: 01011110 01011011 11000111 10010001 ^[.. 00000010: 00101111 00001011 00100011 00101010 /.#* 00000014: 01011110 01011011 11000111 10010001 ^[.. 00000018: 00100111 00101111 10101000 11100000 &#39;/.. ......我们也可以通过 git ls-files --stage 来查看仓库中的每一个文件及其对应的文件对象。 100644 063b0e4ce79bbd23403f7e8ebfb71fb7779f869a 0 .gitignore 100644 3a814e386167aeb4e5bdc1ddc146612c4a42c154 0 package.json ......具体这个文件存储信息的规则是什么样子的由于非常复杂，可以单独展开作为一篇文章讲，这里就不展开说明了。具体的文档可以参考git index-format。 ORIG_HEAD这个文件记录了上一次 HEAD 所在位置的信息。也就是用来反悔用的。 packed-refsgit 会定期执行一个叫做 git gc 的命令，gc 是 garbage collection 的缩写。这个命令会将一些暂时用不到的 commit 和分支的具体内容打包起来，打包在 objects 文件夹下的 pack 文件夹下，用来压缩所占用的体积。这个文件就是用来记录这些信息的。","path":"blog/git/git-文件夹探秘/"},{"title":"JS遍历属性的方法","text":"什么是可枚举属性可枚举是指属性内部可枚举属性没设为 true 的属性。对于通过属性赋值的方式出现的属性来说，默认是可枚举的。对于通过 Object.defineProperty 方式定义的属性默认是不可枚举的。js中的原型属性是不可枚举的。 const a = {} Object.defineProperty(a, &quot;b&quot;, { value: &quot;hello&quot;, // 是否为可枚举属性 enumerable: true, })我们可以看到，使用 defineProperty 方法可以显式的控制设置的属性是否是可枚举属性。 可枚举属性和不可枚举属性在控制台上有十分明显的区别。可枚举属性在控制台上打印出来是正常的颜色，但是不可枚举属性打印出来是有一定透明度的。 遍历可枚举属性的方法 for … in … for in 方法可以遍历对先的每一个可枚举属性，包括了原型链上的可枚举属性 const a = {} a.b = &quot;b&quot; a.__proto__.c = &quot;c&quot; Object.defineProperty(a, &quot;d&quot;, { value: &quot;d&quot;, enumerable: false, }) for(let key in a) { console.log(key) // b, c } Object.keys Object.keys 可以遍历自身对象的所有可枚举属性，区别于 for in ，他不能遍历原型链上的可枚举属性 和它类似的还有 Object.values 和 Object.entries， 他们可以遍历的属性都遵循相同的规则。至少返回值上， Object.values 返回对象的值的数组， Object.entries 返回对象键值对的数组 const a = {} a.b = &quot;b value&quot; a.__proto__.c = &quot;c&quot; Object.defineProperty(a, &quot;d&quot;, { value: &quot;d&quot;, enumerable: false, }) console.log(Object.keys(a)) // [&quot;b&quot;] console.log(Object.values(a)) // [&quot;b value&quot;] console.log(Object.entries(a)) // [[&quot;b&quot;, &quot;b value&quot;]] Object.getOwnPropertyNames Object.getOwnPropertyNames 可以遍历自己对象的所有属性，包括可枚举和不可枚举属性。但是不能遍历自己原型链上的属性。 const a = {} a.b = &quot;b&quot; a.__proto__.c = &quot;c&quot; Object.defineProperty(a, &quot;d&quot;, { value: &quot;d&quot;, enumerable: false, }) const keys = Object.getOwnPropertyNames(a) console.log(keys) // &quot;b&quot; Object.getOwnPropertySymbols Object.getOwnPropertySymbols 可以遍历自己对象上所有以 Symbol 类型的可枚举和不可枚举属性。这里单独拿出来是因为 Symbol 类型的属性的表现十分的特殊。 for in 不可以遍历 Symbol 类型的属性 Object.keys 不可以遍历 Symbol 类型的属性 Object.getOwnPropertyNames 不可以遍历 Symbol 类型的属性 const a = {} const b = Symbol(&quot;b&quot;) a[b] = &quot;b&quot; console.log(Object.getOwnPropertySymbols(a)) // [Symbol(b)] console.log(Object.keys(a)) // [] console.log(Object.getOwnPropertyNames) // [] for(let key in a) { console.log(key) // 没有输出 } Reflect.ownKeys Reflect.ownKeys 直白的说，可以理解为 Object.getOwnPropertyNames(target).concat(Object.getOwnPropertySymbols(target)也就是说，Reflect.ownKeys 可以遍历包括 Symbol 类型的所有课枚举和不可枚举属性 const a = {} a.b = &quot;b&quot; a.__proto__.c = &quot;c&quot; Object.defineProperty(a, &quot;d&quot;, { value: &quot;d&quot;, enumerable: false, }) Object.defineProperty(a, Symbol(&quot;e&quot;), { value: &quot;e&quot;, enumerable: false, }) const keys = Reflect.ownKeys(a) console.log(keys) // [&quot;b&quot;, &quot;d&quot;, Symbol(e)]","path":"blog/js/JS遍历属性的方法/"},{"title":"web component","text":"Web Component 概况Web Component 提供了原生的组件化支持。也就是说，这是可以在不使用 React, Vue, Angular 等框架的情况下，浏览器原生所提供的组件化功能。最新版本的 Chrome, Safari, Edge 都已经对 Web Component 提供了支持。对于那些不支持的版本，同样也可以引入 polyfill 来解决。引入 polyfill 最多可以兼容到 IE11，也就是说，在大多数现代浏览器中都是可以使用 Web Component 的。 Web Component 的基本用法以下的代码都写在 my-button.js 中 const template = document.createElement(&#39;template&#39;); template.innerHTML = ` &lt;div&gt; &lt;button&gt;my button&lt;/button&gt; &lt;/div&gt; `如果我们想要定义一个组件，首先我们要定义他的 HTML 模板。 class Button extends HTMLElement { constructor() { super(); this._shadowRoot = this.attachShadow({ mode: &#39;open&#39; }); this._shadowRoot.appendChild(template.content.cloneNode(true)); } }其次，我们要定义这个组件的内部行为。我们看到，Web Component 的定义采取了类继承的定义方式。继承了 HTMLElement 类。所以在这个类中我们可以使用一切 Element 有的方法。在构造函数中，第一行 attachShadow 获取到了当前组件的根节点(称为 shadowRoot)。这里设置了 mode 为 open，意思是可以直接通过 JS 访问和修改该元素。如果设置为 closed，那么将无法被 JS 访问及修改。例如 video 标签。下一行中，把 template 元素克隆了一下加入到了 HTML 中。注意一定要克隆节点。因为在多次调用这个组件时，如果不克隆而是直接 appendChild 的话，几个组件就会公用这个节点而引起问题。 window.customElements.define(&#39;my-button&#39;, Button);最后这一行代码申明了，当使用 my-button 标签的时候，使用我们刚刚定义的类 Button 来描述他的行为。请注意，为了区分 Web Component 与平常的 HTML 标签，Web Component 强制规定了在申明的标签名中必须要带有 - 符号。 这些代码合在一起，组成了定义一个 Web Component 的文件。 &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;./my-button.js&quot;&gt;&lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;my-button&gt;&lt;/my-button&gt; &lt;/body&gt; &lt;/html&gt;最后在想要使用 Web Component 的地方引入这个文件，并像普通的 HTML 标签一样使用即可。 Web Component 的生命周期与传参Web Component 提供了 connectedCallback 的生命周期。这个生命周期在这个组件被插入 DOM 树后触发。相当于 React 的 componentDidMount， Vue 的 mounted 生命周期。与之对应的是 disconnectCallback，在元素被移出 DOM 数后触发。 // index.html &lt;my-button label=&quot;hi&quot;&gt;&lt;/my-button&gt; // my-button.js static get observedAttributes() { return [&quot;label&quot;] } attributeChangedCallback(attr, oldVal, newVal) { switch(attr) { case &quot;label&quot;: document.querySelector(&quot;button&quot;).innerHTML = newVal } }Web Component 提供了生命周期 attributeChangedCallback 来提示属性的变化。但是只有在 observedAttributes 中注册过的变量才会触发这个生命周期。这个生命周期在组件初始化时会触发，之后在任一注册过的属性发生变化时也会触发。 &lt;my-button&gt;&lt;/my-button&gt; const button = document.querySelector(&quot;my-button&quot;) button.label = &quot;hi&quot;单纯的这样定义和改变一个组件的属性是不会触发 attributeChangedCallback 的生命周期的。我们可以通过使用 getter 和 setter 的方式来控制和获取。 class Button extends HTMLElement { ... set label(value) { this.setAttribute(&quot;label&quot;, value) } get label() { return this.getAtribute(&quot;label&quot;) } attributeChangedCallback(attr, oldVal, newVal) { this.render() } render() { // render HTML } }使用 Web Component 内部的方法// my-button.js class Button extends HTMLElement { ... doSomething() { } } // index.html &lt;my-button&gt;&lt;/my-button&gt; document.querySelector(&quot;my-button&quot;).doSomething()我们可以直接访问我们定义的 Web Component 下定义的方法。 Web Component 的事件// my-button.js connectedCallback() { this.$button.addEventListener(&quot;click&quot;, () =&gt; { this.$button.dispatchEvent( new CustomEvent(&quot;test&quot;, { detail: &quot;hello, composed: true, }) ); }) } // index.html document.querySelector(&quot;my-button&quot;).addEventListener(&quot;test&quot;, value =&gt; { console.log(&quot;test&quot;, value) // CustomEvent Object })当想要向外触发事件的时候，先使用 CustomEvent 自定义一个事件，其中 detail 字段可以是任意值，为向外传递的信息。必须设置 composed: true，否则将无法在组件外监听到信息。然后使用 dispatchEvent 在组件的一个 HTML 元素上触发事件。这样就可以在外界监听到这个事件。 Shadow DOM你所定义的 Web Component 全部都封装在 Shadow Dom 中。你在其中定义的 CSS 将不会污染外部的 CSS。但是你可以通过外部来改变 Shadow DOM 内部的样式。推荐的方法是在 Shadow DOM 内部使用 CSS 变量，然后在外部定义不同的 CSS 变量值来控制内部的样式。所有的自定义组件的元素将被挂载在一个被称为 Shadow Root 的根元素上。这个元素的 css 选择器为 :host。 Web Component 的 slotWeb Component 中的 slot 和 Vue 中的 slot 形式非常相似。 // my-button.js 的 template 定义部分 &lt;div&gt; &lt;button&gt; &lt;slot&gt;&lt;/slot&gt; &lt;/button&gt; &lt;slot name=&quot;after&quot;&gt;&lt;/slot&gt; &lt;/div&gt; // index.html &lt;my-button&gt; &lt;div&gt;hello&lt;/div&gt; &lt;div slot=&quot;after&quot;&gt;after&lt;/div&gt; &lt;/my-button&gt; // 实际渲染 &lt;div&gt; &lt;button&gt; &lt;slot&gt; &lt;div&gt;hello&lt;/div&gt; &lt;/slot&gt; &lt;/button&gt; &lt;slot name=&quot;after&quot;&gt; &lt;div&gt;after&lt;/div&gt; &lt;/slot&gt; &lt;/div&gt;有默认的 slot 和具名的 slot，他们分别将插入在各自对应的 slot 的内部。slot 也有他自己的 CSS 选择器： ::slot(after) 指名字为 after 的 slot 的 CSS 选择器。 扩展原生元素当你想让你的元素继承一个原生元素所有的特性时，你可以这样定义你的组件。 class MyButton extends HTMLButtonElement { ... } customElements.define(&quot;my-button&quot;, MyButton, { extends: &quot;button&quot; });只需要在继承时继承那个原生组件的类，并且在 define 时的第三个属性中申明 extends 的原生标签的名字。这是就可以使用这个元素。这个元素可以通过 is 属性来使用。 &lt;button is=&quot;my-button&quot;&gt;&lt;/button&gt;但是在扩展元素时不能尝试创建 Shadow Root。如果尝试创建 Shadow Root 将会报错。 扩展原生元素的另一个好处是在子元素被限制的时候也可以使用。例如 thead 的子元素必须是 tr。这时如果你想使用一个自己定义的 Web Component 作为子元素的话是不行的，只有使用这种扩展原生元素的方法，写成 &lt;tr is=&quot;my-tr&quot;&gt;&lt;/tr&gt; 才能正常使用。","path":"blog/HTML/web-component/"},{"title":"git不常用命令整理（1）","text":"本文是对于一些不是很常用的 git 命令的整理，让我们想要用到一些 git 不常用的功能时，心里可以有一点逼数，知道哪些是可以做的，哪些是做不到的。 基础的 git 命令在写不常用的命令前，这里是我整理的一些比较常用的一些 git 命令。如果发现自己不认识的话就自己去谷歌一下补补课吧。 git init git config git clone git add git status git diff git commit git mv git rm git reset git branch git checkout git merge git log git stash git tag git fetch git pull git push git remote git cherry-pick git rebase git revert git help 不常用的 git 命令以下是一些不常用的 git 命令。由于这是一篇总结性的文章，所以这里只会简单的说明每个命令的功能，让大家知道 git 可以干些什么。如果想要知道更加详细具体的用法，或者遇到了某些不知道含义的名词，可以去查看 git 文档。 git archive: 将整个项目打包并压缩成一个文件，不包含 git 历史 git bisect: 在项目遇到 bug 时，用这个命令可以帮助你用二分查找法的方式迅速定位是哪一个 commit 引入了这个 bug git bundle: 将整个项目打包成一个文件，包含 git 历史 git citool: 这是一个图形化 git commit 命令，是 git gui citool 的 alias git clean: 删除所有不在暂存区的文件或目录 git describe: 查找从提交可访问的最新标记。 如果标签指向提交，则只显示标签。 否则，它将标记名称与标记对象之上的其他提交数量以及最近提交的缩写对象名称后缀 git format-patch: 打包出一个包含 commit 信息的 patch 文件。可以只用 git am 把打包文件应用于另一个 git 仓库。类似于使用 git diff 生成的 patch 文件，只不过多了 commit 信息。 git gc: gc 是 garbage collection 的缩写，主要是用于优化磁盘空间 git grep: 用于在 git 历史中进行搜索 git gui: 展示 git 的 gui 界面 gitk: 展示 git 图形化操作工具 git notes: 为提交添加评注 git range-diff: 展示两组提交之间的 diff。比如我在master上提交了一些 commit，你也在master上提交了一些 commit（由于没有 push，没冲突），对你我提交的内容进行 diff git restore: 类似于 git checkout -- git shortlog: 输出所有的git 的 commit msg，以用户进行分组 git show: 展示一个或者多个对象。如 git show HEAD 是展示当前最新提交的 diff 信息 git sparse-checkout: 只下载远程仓库中某一个特定文件夹 git submodule: 在一个 git 仓库中允许建立另一个 git 仓库。但是可以保持各自的工作流独立。常用于多个项目复用同一个包。 git switch: 切换分支，相当于 git checkout git worktree: 在存在一个 git 仓库的情况下，在另一个文件夹下创建这个项目某一个分支的代码，但是新的文件夹下的 .git 并不会维护全部的代码历史，只会标明指向原来的项目。这样可以节省磁盘空间。也省了切换分支的开销。常用于大型项目。 git fast-export: 将提交导出为 git-fast-import 格式 git fast-import: 其他版本库迁移至Git的通用工具 TODO， 敬请期待下一期 git filter-branch: git mergetool: git pack-refs: git repack: git replace: git annotate: git blame: git bugreport: git count-objects: git difftool: git fsck: git instaweb: git merge-tree: git rerere: git show-branch: git verify-commit: git verify-tag: git whatchanged: git archimport: git cvsexportcommit: git cvsimport: git cvsserver: git imap-send: git p4: git quiltimport: git request-pull: git send-email: git svn: git apply: git checkout-index: git commit-graph: git commit-tree: git hash-object: git index-pack: git merge-file: git merge-index: git multi-pack-index: git mktag: git mktree: git pack-objects: git prune-packed: git read-tree: git symbolic-ref: git unpack-objects: git update-index: git update-ref: git write-tree: git cat-file: git cherry: git diff-files: git diff-index: git diff-tree: git for-each-ref: git get-tar-commit-id: git ls-files: git ls-remote: git ls-tree: git merge-base: git name-rev: git pack-redundant: git rev-list: git rev-parse: git show-index: git show-ref: git unpack-file: git var: git verify-pack: git daemon: git fetch-pack: git http-backend: git send-pack: git update-server-info: git http-fetch: git http-push: git parse-remote: git receive-pack: git shell: git upload-archive: git upload-pack: git check-attr: git check-ignore: git check-mailmap: git check-ref-format: git column: git credential: git credential-cache: git credential-store: git fmt-merge-msg: git interpret-trailers: git mailinfo: git mailsplit: git merge-one-file: git patch-id: git sh-i18n: git sh-setup: git stripspace: reference git reference","path":"blog/git/git不常用命令整理（1）/"},{"title":"JS包管理机制","text":"最初的JS模块化最初的JS是不存在规范化的模块化管理的，所有的函数和变量会直接污染全局的命名空间。最初人们的解决方法是使用立即执行函数(IIFE: Immediately Invoked Function Expression)进行模块化。 // 全局变量 var global = &quot;hello world&quot; (function() { // 只能在闭包内访问的变量 var a = 1 })()但是仅仅这样是不够的。2009年 Mozilla 工程师 Kevin Dangoor 发起了 ServerJS 项目（后更名为CommonJS），后被我们熟知的 Node.js 使用作为模块管理的规范，也就是后来广为人知的 CommonJS 规范。 CommonJSCommonJS 使用入如下语法定义一个模块。 CommonJS 暴露了 module.exports 作为了对外界的接口。CommonJS 同时定义了 exports 作为了 module.exports 的引用。同样可以使用他来作为输出对象。 // module.js const a = 1 function b() { console.log(&quot;hello world&quot;) } module.exports = { b: b, } exports.a = a这时候，如果想在另一个模块引用这些函数以及变量 // main.js const a = require(&quot;module&quot;).a const b = require(&quot;module&quot;).b如果想让模块直接暴露一个函数或变量，可以这么写 // module.js const a = 1 module.exports = a这时候如果想要使用这个变量 // main.js const a = require(&quot;module&quot;) // 1但是，如果你抖一个小机灵说，既然同样可以使用 exports 来输出对象，那我这样写怎么样？ // module.js const a = 1 exports = a这样是不行的。因为之前说过， exports 是作为 module.exports 的引用存在的，你这样写相当于改变了 exports 的指向。程序对外暴露的只是 module.exports，这时的 exports 当然无法生效了。看如下代码你就明白了。 const module = { exports: {}, } const exports = module.exports // 以上代码是程序的预定义部分，将 module.exports 向外暴露 const a = 1 exports = a // 这时 module.exports 仍然为空对象CommonJS 致力于服务端的生态。但是对于浏览器端 CommonJS 是无法接受的。在服务端，在引用一个模块时，从磁盘中读取一个模块的延时是很短的，但是在浏览器端，网络的延迟导致了模块的加载可能会阻碍其他代码的运行。所以 CommonJS 是无法被浏览器端所接受的。这时候， AMD 规范也就应运而生。 AMDAMD 规范采用异步方式加载模块，模块的加载不影响它后面语句的运行。所有依赖这个模块的语句，都定义在一个回调函数中，等到加载完成之后，这个回调函数才会运行。require.js 是最常见的实现 AMD 规范的库。 require.js 的 API 是这样子的 define(id?, dependencies?, factory)第一个参数是这个模块的名字。第二个参数是这个模块的依赖。第三个参数是定义这个模块的代码主题。如果你想要定义的模块存在依赖，定义的回调函数将在所有的依赖加载完，并且初始化完成后加载。 // define 第一个参数是依赖列表，分别对应第二个参数回调函数的每一个函数参数 define([&quot;a&quot;, &quot;jquery&quot;, &quot;c&quot;], function(a, $, c)) { $(&quot;body&quot;).innerHTML = &quot;hello world&quot; a.doSomething() c.doSomething() }如果你想引用一个模块，第一个参数是依赖列表，第二个参数是所有模块加载完后的回调函数 require([&quot;jquery&quot;], function($) { $(&quot;body&quot;).innerHTML = &quot;hello world&quot; })但是 玉伯 认为 RequireJS 不够完善，于是就催生了 CMD CMDSeaJS 是实现 CMD 规范的一个库。 不同于 AMD 推崇的依赖前置, CMD 推崇的是依赖就近。他们俩都会提前把依赖下载下载下来，但是不同的是 AMD 在下载后悔立即执行依赖模块的代码，但是 CMD 只有在你实际使用的时候，也就是你调用 require 的时候才会执行被你所 require 的模块的代码。这两种模式的优劣社区中有很多中看法。仁者见仁，智者见智。 SeaJS 的 API 是这样子的 define(function(require, exports, module) { var $ = require(&quot;jquery.js&quot;) $(&quot;body&quot;).innerHTML = &quot;hello world&quot; });ES6 MODULE之前提到的几种模块化标准都是不是一种标准，而是由社区提出并广受大众接受的一种方案。而 ES6 MODULE 则是在语言标准层面实现的模块化。目前所有的现代浏览器都已经支持了这种语法。 // 导出早前定义的函数 const myFunction = () =&gt; {} export { myFunction } // 或者作为默认导出，相当于 export { myFunction as default } 的语法糖 export default myFunction // 导入模块 import { foo, bar } from &quot;module.js&quot; // 导入默认模块，相当于 import { default as foo } from &quot;module.js&quot; 的语法糖 import foo from &quot;module.js&quot;这种声明式而非命令式的语法让静态分析成为了可能。在编译时和写代码时带来了极大的益处。 和 CommonJS 不同的是， import 导出的是一个引用，而 require 导出的是一个拷贝。也就是说，在 CommonJS 中如果改变一个模块内部的值是不会对引用他的地方的值造成影响的，但是 ES6 MODULE 是有影响的。举一个简单的例子。 // 对于 CommonJS // module.js let a = 1 const adder = () =&gt; a++ module.export = { a, adder, } // main.js const { a, adder } = require(&quot;module.js&quot;) console.log(a) // 1 adder() console.log(a) // 1// 对于 ES6 MODULE // module.js let a = 1 const adder = () =&gt; a++ export { a, adder } // main.js import { a, adder } from &quot;module.js&quot; console.log(a) // 1 adder() console.log(a) // 2UMDUMD 其实是一种利用 IIFE(立即执行函数) 来兼容 AMD 和 CommonJS 的一种形式。使你的代码可以在多个不同平台上，不同的模块化标准都可以运行。 (function(global, factory) { // 判断是 CommonJS 环境 typeof exports === &#39;object&#39; &amp;&amp; typeof module !== &#39;undefined&#39; ? module.exports = factory() : // 判断使用 AMD 标准，如果想要兼容 CMD，多加一条判断 define.cmd 即可 typeof define === &#39;function&#39; &amp;&amp; define.amd ? define(factory) : // 否则就作为全局变量挂在当前环境的 this 上 (global.NAME = factory()); }(this, (function () { // 你自己定义的模块 return obj; })));Webpack 中的模块化打包那么在现在大家常用的打包工具 Webpack 中，模块化是如何进行编译处理的呢？让我们看一个例子。节选自 lq782655835/webpack-module-example // src/add.js export default function(a, b) { let { name } = { name: &#39;hello world,&#39;} return name + a + b } // src/main.js import Add from &#39;./add&#39; console.log(Add, Add(1, 2))// modules是存放所有模块的数组，数组中每个元素存储{ 模块路径: 模块导出代码函数 } (function(modules) { // 模块缓存作用，已加载的模块可以不用再重新读取，提升性能 var installedModules = {}; // 关键函数，加载模块代码 // 形式有点像Node的CommonJS模块，但这里是可跑在浏览器上的es5代码 function __webpack_require__(moduleId) { // 缓存检查，有则直接从缓存中取得 if(installedModules[moduleId]) { return installedModules[moduleId].exports; } // 先创建一个空模块，塞入缓存中 var module = installedModules[moduleId] = { i: moduleId, l: false, // 标记是否已经加载 exports: {} // 初始模块为空 }; // 把要加载的模块内容，挂载到module.exports上 modules[moduleId].call(module.exports, module, module.exports, __webpack_require__); module.l = true; // 标记为已加载 // 返回加载的模块，调用方直接调用即可 return module.exports; } // __webpack_require__对象下的r函数 // 在module.exports上定义__esModule为true，表明是一个模块对象 __webpack_require__.r = function(exports) { Object.defineProperty(exports, &#39;__esModule&#39;, { value: true }); }; // 启动入口模块main.js return __webpack_require__(__webpack_require__.s = &quot;./src/main.js&quot;); }) ({ // add模块 &quot;./src/add.js&quot;: (function(module, __webpack_exports__, __webpack_require__) { // 在module.exports上定义__esModule为true __webpack_require__.r(__webpack_exports__); // 直接把add模块内容，赋给module.exports.default对象上 __webpack_exports__[&quot;default&quot;] = (function(a, b) { let { name } = { name: &#39;hello world,&#39;} return name + a + b }); }), // 入口模块 &quot;./src/main.js&quot;: (function(module, __webpack_exports__, __webpack_require__) { __webpack_require__.r(__webpack_exports__) // 拿到add模块的定义 // _add__WEBPACK_IMPORTED_MODULE_0__ = module.exports，有点类似require var _add__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(&quot;./src/add.js&quot;); // add模块内容: _add__WEBPACK_IMPORTED_MODULE_0__[&quot;default&quot;] console.log(_add__WEBPACK_IMPORTED_MODULE_0__[&quot;default&quot;], Object(_add__WEBPACK_IMPORTED_MODULE_0__[&quot;default&quot;])(1, 2)) }) });reference前端模块化：CommonJS,AMD,CMD,ES6Webpack 模块打包原理","path":"blog/js/JS包管理机制/"}],"categories":[],"tags":[]}